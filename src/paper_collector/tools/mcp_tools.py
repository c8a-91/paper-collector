"""
Model Context Protocol (MCP) ãƒ„ãƒ¼ãƒ«é–¢æ•°ã‚’å®šç¾©ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
è«–æ–‡ã®æ¤œç´¢ã€è¡¨ç¤ºã€ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãªã©ã®æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã€‚
"""
from typing import Any, List, Dict, Optional
import os
import json
import pandas as pd
import asyncio
from mcp.server.fastmcp import FastMCP
from datetime import datetime
from pathlib import Path

try:
    # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã—ã¦å®Ÿè¡Œã™ã‚‹å ´åˆ
    from paper_collector.db.database import PaperDatabase
    from paper_collector.pdf.pdf_handler import extract_text_from_pdf
    from paper_collector.api.arxiv_client import ArxivClient
    from paper_collector.api.semantic_scholar_client import SemanticScholarClient
    from paper_collector.utils.file_utils import ensure_directory_exists
    from paper_collector.utils.config import config
except ImportError:
    # é–‹ç™ºç’°å¢ƒã§ç›´æ¥å®Ÿè¡Œã™ã‚‹å ´åˆï¼ˆç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
    from ..db.database import PaperDatabase
    from ..pdf.pdf_handler import extract_text_from_pdf
    from ..api.arxiv_client import ArxivClient
    from ..api.semantic_scholar_client import SemanticScholarClient
    from ..utils.file_utils import ensure_directory_exists
    from ..utils.config import config

# MCPã‚µãƒ¼ãƒãƒ¼ã®åˆæœŸåŒ–
mcp = FastMCP("paper-collector")

# è¨­å®šã‹ã‚‰ãƒ‘ã‚¹ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
DATA_DIR = config.get_data_dir()
PAPERS_DIR = config.get_papers_dir()
DB_PATH = config.get_db_path()
API_DELAY = config.get("api_delay", 1.0)

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ã‚’ç¢ºèª
ensure_directory_exists(DATA_DIR)
ensure_directory_exists(PAPERS_DIR)

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
paper_db = PaperDatabase(DB_PATH)
arxiv_client = ArxivClient(PAPERS_DIR, API_DELAY)
semantic_scholar_client = SemanticScholarClient(PAPERS_DIR, API_DELAY)

# ãƒ­ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
def log_info(message: str) -> None:
    """æƒ…å ±ãƒ­ã‚°ã‚’å‡ºåŠ›ã—ã¾ã™"""
    print(f"[INFO] {message}")

def log_error(message: str) -> None:
    """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‡ºåŠ›ã—ã¾ã™"""
    print(f"[ERROR] {message}")

@mcp.tool()
async def search_papers(query: str, source: str = "both", limit: int = 5) -> str:
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹è«–æ–‡æ¤œç´¢ã‚’è¡Œã„ã€çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã™ã€‚
    
    è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’çµ„ã¿åˆã‚ã›ãŸæ¤œç´¢ãŒå¯èƒ½ã§ã™ã€‚æ¤œç´¢æ§‹æ–‡ã®ã‚µãƒãƒ¼ãƒˆã¯æ¤œç´¢ã‚½ãƒ¼ã‚¹ã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ã€‚
    æ¤œç´¢çµæœã¯è‡ªå‹•çš„ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã€å¾Œã§å‚ç…§ã§ãã¾ã™ã€‚
    
    æ¤œç´¢ã®ãƒ’ãƒ³ãƒˆ:
    - arXivæ¤œç´¢ã§ã¯ä»¥ä¸‹ã®é«˜åº¦ãªæ¤œç´¢æ§‹æ–‡ãŒä½¿ç”¨å¯èƒ½ã§ã™:
      * è¤‡æ•°ã®å˜èªã¯è‡ªå‹•çš„ã«ANDæ¤œç´¢ã•ã‚Œã¾ã™ (ä¾‹: "quantum computing" ã¯ "quantum" AND "computing")
      * ãƒ•ãƒ¬ãƒ¼ã‚ºæ¤œç´¢ã«ã¯å¼•ç”¨ç¬¦ã‚’ä½¿ç”¨: "quantum computing"
      * ORæ¤œç´¢: "machine learning" OR "deep learning" 
      * é™¤å¤–æ¤œç´¢: "neural networks" ANDNOT "convolutional"
      * ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æŒ‡å®šæ¤œç´¢: ti:transformerï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã«"transformer"ã‚’å«ã‚€ï¼‰ã€au:bengioï¼ˆè‘—è€…ã«"bengio"ã‚’å«ã‚€ï¼‰
      * arXivã§ã¯ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆ"neura*"ãªã©ï¼‰ã¯å…¬å¼ã«ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“
    - Semantic Scholaræ¤œç´¢ã§ã¯:
      * åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®ã¿ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™
      * ãƒ–ãƒ¼ãƒ«æ¼”ç®—å­(AND, OR, NOT)ã‚„é«˜åº¦ãªæ§‹æ–‡ã¯å…¬å¼ã«ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“
      * æ¤œç´¢ã¯å˜ç´”ãªãƒ†ã‚­ã‚¹ãƒˆä¸€è‡´ã§è¡Œã‚ã‚Œã€è¤‡æ•°å˜èªã¯è‡ªå‹•çš„ã«ANDã¨ã—ã¦æ‰±ã‚ã‚Œã¾ã™
      * å¹´(year)ã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿ãŒå¯èƒ½ã§ã™
    
    Args:
        query: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã€‚arXivã§ã¯é«˜åº¦ãªæ¤œç´¢æ§‹æ–‡ã‚‚ã‚µãƒãƒ¼ãƒˆ
        source: è«–æ–‡ã‚½ãƒ¼ã‚¹ ("arxiv", "semantic_scholar", ã¾ãŸã¯ "both")
        limit: å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°ã€‚åˆè¨ˆä»¶æ•°ã¯sourceãŒ"both"ã®å ´åˆã€æœ€å¤§ã§limit*2ã«ãªã‚Šã¾ã™
    
    Returns:
        æ¤œç´¢çµæœã®è¦ç´„ã€‚è¦‹ã¤ã‹ã£ãŸè«–æ–‡æ•°ã€æ–°è¦è¿½åŠ ã•ã‚ŒãŸè«–æ–‡æ•°ã€å…¨æ–‡ãŒåˆ©ç”¨å¯èƒ½ãªè«–æ–‡æ•°ãªã©ã‚’å«ã¿ã¾ã™
    
    ä¾‹:
        * search_papers("attention mechanism") - æ³¨æ„æ©Ÿæ§‹ã«é–¢ã™ã‚‹è«–æ–‡ã‚’æ¤œç´¢
        * search_papers("\"transformer architecture\" ANDNOT BERT", source="arxiv", limit=10) - 
          BERTã‚’é™¤ããƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«é–¢ã™ã‚‹è«–æ–‡ã‚’arXivã‹ã‚‰æœ€å¤§10ä»¶æ¤œç´¢
        * search_papers("reinforcement learning robotics", source="semantic_scholar") - 
          å¼·åŒ–å­¦ç¿’ã¨ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã«é–¢ã™ã‚‹è«–æ–‡ã‚’Semantic Scholarã‹ã‚‰æ¤œç´¢
        * search_papers("ti:\"attention is all you need\"", source="arxiv") - 
          ã‚¿ã‚¤ãƒˆãƒ«ã«ã€ŒAttention is All You Needã€ã‚’å«ã‚€è«–æ–‡ã‚’arXivã‹ã‚‰æ¤œç´¢
        * search_papers("au:hinton", limit=15) - HintonãŒè‘—è€…ã®è«–æ–‡ã‚’æœ€å¤§15ä»¶æ¤œç´¢
    """
    try:
        papers = []
        
        if source.lower() in ["arxiv", "both"]:
            log_info(f"ArXivã§æ¤œç´¢: {query}")
            arxiv_papers = await arxiv_client.search(query, limit)
            papers.extend(arxiv_papers)
            
        if source.lower() in ["semantic_scholar", "both"]:
            log_info(f"Semantic Scholarã§æ¤œç´¢: {query}")
            semantic_papers = await semantic_scholar_client.search(query, limit)
            papers.extend(semantic_papers)
        
        saved_count = paper_db.save_papers(papers)
        
        summary = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{query}ã€ã§{len(papers)}ä»¶ã®è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n"
        summary += f"ãã®ã†ã¡{saved_count}ä»¶ãŒæ–°è¦ã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚\n\n"
        
        full_text_count = sum(1 for paper in papers if paper.get("full_text_available", 0) == 1)
        summary += f"å…¨æ–‡ãŒåˆ©ç”¨å¯èƒ½ãªè«–æ–‡: {full_text_count}ä»¶\n\n"
        
        if saved_count > 0:
            summary += "æ–°è¦è¿½åŠ ã•ã‚ŒãŸè«–æ–‡:\n"
            seen_ids = set()
            
            for paper in papers:
                paper_id = paper["paper_id"]
                if paper_id not in seen_ids:
                    seen_ids.add(paper_id)
                    
                    db_paper = paper_db.get_paper_by_id(paper_id)
                    if db_paper and db_paper["collected_date"] == datetime.now().strftime("%Y-%m-%d"):
                        full_text = "ï¼ˆå…¨æ–‡ã‚ã‚Šï¼‰" if paper.get("full_text_available", 0) == 1 else ""
                        summary += f"- {paper['title']} ({paper['source']}) {full_text}\n"
        
        return summary
    except Exception as e:
        log_error(f"è«–æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"è«–æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def search_papers_by_citations(
    query: str, 
    min_citations: int = 0, 
    source: str = "both", 
    limit: int = 5, 
    sort_by: str = "citations"
) -> str:
    """
    å¼•ç”¨æ•°ã‚’è€ƒæ…®ã—ãŸè«–æ–‡æ¤œç´¢ã‚’è¡Œã„ã€çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã™ã€‚
    
    é«˜å¼•ç”¨æ•°ã®è«–æ–‡ï¼ˆå½±éŸ¿åŠ›ã®é«˜ã„è«–æ–‡ï¼‰ã‚’å„ªå…ˆçš„ã«è¦‹ã¤ã‘ã‚‹ãŸã‚ã®æ¤œç´¢æ©Ÿèƒ½ã§ã™ã€‚
    é€šå¸¸ã®æ¤œç´¢ã¨åŒæ§˜ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢æ§‹æ–‡ãŒä½¿ç”¨ã§ãã¾ã™ãŒã€ã•ã‚‰ã«å¼•ç”¨æ•°ã«ã‚ˆã‚‹
    ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ã‚½ãƒ¼ãƒˆãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚æ¤œç´¢çµæœã¯è‡ªå‹•çš„ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«
    ä¿å­˜ã•ã‚Œã€å¾Œã§å‚ç…§ã§ãã¾ã™ã€‚
    
    æ¤œç´¢ã®ãƒ’ãƒ³ãƒˆ:
    - å¼•ç”¨æ•°ã¯è«–æ–‡ã®å½±éŸ¿åŠ›ã‚„é‡è¦æ€§ã®æŒ‡æ¨™ã¨ã—ã¦åˆ©ç”¨ã§ãã€è¢«å¼•ç”¨æ•°ã®é«˜ã„åŸºç¤è«–æ–‡ã‚„
      é‡è¦è«–æ–‡ã‚’ç´ æ—©ãè¦‹ã¤ã‘ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™
    - "citations"ã§ã‚½ãƒ¼ãƒˆã™ã‚‹ã¨ã€æœ€ã‚‚å¼•ç”¨æ•°ã®å¤šã„è«–æ–‡ã‹ã‚‰é †ã«è¡¨ç¤ºã•ã‚Œã¾ã™
    - "recency"ã§ã‚½ãƒ¼ãƒˆã™ã‚‹ã¨ã€æœ€æ–°ã®è«–æ–‡ã‹ã‚‰é †ã«è¡¨ç¤ºã•ã‚Œã€æœ€è¿‘ã®ç ”ç©¶å‹•å‘ãŒã‚ã‹ã‚Šã¾ã™
    - ArXivã¨Semantic Scholarã‹ã‚‰è«–æ–‡ã‚’æ¤œç´¢ã—ã€APIã‹ã‚‰å–å¾—ã•ã‚ŒãŸå¼•ç”¨æ•°æƒ…å ±ã‚’ä½¿ç”¨ã—ã¾ã™
    - åŸå‰‡ã¨ã—ã¦ArXiv IDã¯ä¿æŒã•ã‚Œã‚‹ãŸã‚ã€åŒã˜è«–æ–‡ãŒä¸¡æ–¹ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢ã•ã‚Œã¦ã‚‚
      é‡è¤‡ã—ã¦è¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“
    
    Args:
        query: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã€‚arXivã§ã¯ANDã‚„ORãªã©ã®é«˜åº¦ãªæ¤œç´¢æ§‹æ–‡ã‚‚ã‚µãƒãƒ¼ãƒˆ
        min_citations: æœ€å°å¼•ç”¨æ•°ã€‚ã“ã‚Œã‚ˆã‚Šå¼•ç”¨æ•°ãŒå°‘ãªã„è«–æ–‡ã¯çµæœã‹ã‚‰é™¤å¤–ã•ã‚Œã¾ã™
        source: è«–æ–‡ã‚½ãƒ¼ã‚¹ ("arxiv", "semantic_scholar", ã¾ãŸã¯ "both")
        limit: å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
        sort_by: ã‚½ãƒ¼ãƒˆæ–¹æ³• ("relevance"=é–¢é€£æ€§é †, "citations"=å¼•ç”¨æ•°é †, "recency"=æ—¥ä»˜é †)
    
    Returns:
        æ¤œç´¢çµæœã®è¦ç´„ã€‚è¦‹ã¤ã‹ã£ãŸè«–æ–‡æ•°ã€æ–°è¦è¿½åŠ ã•ã‚ŒãŸè«–æ–‡æ•°ã€å„è«–æ–‡ã®å¼•ç”¨æ•°ã‚„æ²è¼‰å…ˆãªã©ã‚’å«ã¿ã¾ã™
    
    ä¾‹:
        * search_papers_by_citations("transformer", min_citations=1000) - 
          1000å›ä»¥ä¸Šå¼•ç”¨ã•ã‚ŒãŸTransformeré–¢é€£ã®è«–æ–‡ã‚’æ¤œç´¢ï¼ˆAttention is All Youãªã©ï¼‰
        * search_papers_by_citations("BERT language model", min_citations=500, source="semantic_scholar") - 
          Semantic Scholarã‹ã‚‰500å›ä»¥ä¸Šå¼•ç”¨ã•ã‚ŒãŸBERTè¨€èªãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹è«–æ–‡ã‚’æ¤œç´¢
        * search_papers_by_citations("deep reinforcement learning", sort_by="recency") - 
          æ·±å±¤å¼·åŒ–å­¦ç¿’ã«é–¢ã™ã‚‹è«–æ–‡ã‚’æœ€æ–°é †ã«ã‚½ãƒ¼ãƒˆã—ã¦è¡¨ç¤ºï¼ˆå¼•ç”¨æ•°ã®åˆ¶é™ãªã—ï¼‰
        * search_papers_by_citations("\"attention is all you need\"", min_citations=5000) - 
          æœ‰åãªTransformerè«–æ–‡ã‚’æ¤œç´¢ï¼ˆæ­£ç¢ºãªã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢ã¨é«˜ã„å¼•ç”¨æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ç‰¹å®šã®è«–æ–‡ã‚’è¦‹ã¤ã‘ã‚‹ï¼‰
        * search_papers_by_citations("au:hinton", min_citations=200) - 
          HintonãŒè‘—è€…ã§200å›ä»¥ä¸Šå¼•ç”¨ã•ã‚ŒãŸè«–æ–‡ã‚’æ¤œç´¢
    """
    try:
        papers = []
        
        if source.lower() in ["arxiv", "both"]:
            log_info(f"ArXivã§å¼•ç”¨æ•°æ¤œç´¢: {query} (æœ€å°å¼•ç”¨æ•°: {min_citations})")
            arxiv_papers = await arxiv_client.search(query, limit, min_citations, sort_by)
            papers.extend(arxiv_papers)
            
        if source.lower() in ["semantic_scholar", "both"]:
            log_info(f"Semantic Scholarã§å¼•ç”¨æ•°æ¤œç´¢: {query} (æœ€å°å¼•ç”¨æ•°: {min_citations})")
            semantic_papers = await semantic_scholar_client.search(query, limit, min_citations, sort_by)
            papers.extend(semantic_papers)
        
        if sort_by == "citations":
            papers.sort(key=lambda x: x.get("citation_count", 0), reverse=True)
            papers = papers[:limit]
        
        saved_count = paper_db.save_papers(papers)
        
        summary = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{query}ã€ã§æœ€å°å¼•ç”¨æ•° {min_citations} ä»¥ä¸Šã®è«–æ–‡ãŒ {len(papers)}ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n"
        summary += f"ãã®ã†ã¡{saved_count}ä»¶ãŒæ–°è¦ã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚\n\n"
        
        if papers:
            summary += "æ¤œç´¢çµæœ:\n"
            for i, paper in enumerate(papers, 1):
                full_text = "ï¼ˆå…¨æ–‡ã‚ã‚Šï¼‰" if paper.get("full_text_available", 0) == 1 else ""
                summary += f"{i}. {paper['title']} ({paper['source']})\n"
                summary += f"   è‘—è€…: {paper['authors']}\n"
                summary += f"   å¼•ç”¨æ•°: {paper.get('citation_count', 0)}\n"
                if paper.get("venue"):
                    summary += f"   æ²è¼‰å…ˆ: {paper.get('venue')}\n"
                summary += f"   URL: {paper['url']}\n"
                summary += f"   {full_text}\n\n"
        else:
            summary += "æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ã«åˆè‡´ã™ã‚‹è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        return summary
    except Exception as e:
        log_error(f"å¼•ç”¨æ•°ã«ã‚ˆã‚‹è«–æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"å¼•ç”¨æ•°ã«ã‚ˆã‚‹è«–æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def list_saved_papers(
    keyword: str = "", 
    source: str = "", 
    limit: int = 10, 
    sort_by: str = "date", 
    sort_order: str = "desc",
    filter_has_fulltext: bool = False,
    min_citations: int = 0,
    format: str = "detailed",
    venue: str = "",
    date_from: str = "",
    date_to: str = ""
) -> str:
    """
    ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã®ä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚å¤šæ§˜ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ã‚½ãƒ¼ãƒˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹è«–æ–‡ã‚’æ¤œç´¢ã—ã€ã•ã¾ã–ã¾ãªæ¡ä»¶ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦
    è¡¨ç¤ºã§ãã¾ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€å‡ºç‰ˆå…ƒã€æ—¥ä»˜ç¯„å›²ã€å¼•ç”¨æ•°ãªã©è¤‡æ•°ã®æ¡ä»¶ã‚’çµ„ã¿åˆã‚ã›ã¦
    æ¤œç´¢ã§ãã‚‹å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
    
    ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ã‚½ãƒ¼ãƒˆã®ãƒ’ãƒ³ãƒˆ:
    - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¯éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼ˆSQLã®LIKEæ¼”ç®—å­ã‚’ä½¿ç”¨ï¼‰
      * ã€Œtransformerã€ã§æ¤œç´¢ã™ã‚‹ã¨ã€ã€Œtransformerã€ã€Œtransformersã€ã€Œtransformer-basedã€ãªã©ã‚’å«ã‚€ã™ã¹ã¦ã®è«–æ–‡ãŒãƒãƒƒãƒã—ã¾ã™
      * è¤‡æ•°ã®å˜èªï¼ˆä¾‹: ã€Œneural networkã€ï¼‰ã§æ¤œç´¢ã™ã‚‹ã¨ã€ä¸¡æ–¹ã®å˜èªã‚’å«ã‚€è«–æ–‡ãŒãƒãƒƒãƒã—ã¾ã™
      * ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã‚¿ã‚¤ãƒˆãƒ«ã€æ¦‚è¦ã€è‘—è€…åã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã„ãšã‚Œã‹ã«å«ã¾ã‚Œã¦ã„ã‚Œã°ãƒãƒƒãƒã—ã¾ã™
    - venueï¼ˆæ²è¼‰å…ˆï¼‰ã®æ¤œç´¢ã‚‚åŒæ§˜ã«éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™
      * ã€ŒACLã€ã§æ¤œç´¢ã™ã‚‹ã¨ã€ã€ŒACL 2023ã€ã€ŒTACLã€ã€ŒNAACLã€ãªã©ã‚’å«ã‚€è«–æ–‡ãŒãƒãƒƒãƒã—ã¾ã™
    - æ—¥ä»˜ç¯„å›²ã‚’æŒ‡å®šã™ã‚‹å ´åˆã¯YYYY-MM-DDå½¢å¼ï¼ˆä¾‹: 2023-01-01ï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™
    - ã‚½ãƒ¼ãƒˆé †ã¯ "date"ï¼ˆæ—¥ä»˜é †ï¼‰ã€"citations"ï¼ˆå¼•ç”¨æ•°é †ï¼‰ã€"title"ï¼ˆã‚¿ã‚¤ãƒˆãƒ«é †ï¼‰ã‹ã‚‰é¸æŠã§ãã¾ã™
    - è¡¨ç¤ºå½¢å¼ã¯ "detailed"ï¼ˆè©³ç´°ï¼‰ã€"compact"ï¼ˆç°¡ç•¥ï¼‰ã€"csv"ï¼ˆCSVå½¢å¼ï¼‰ã‹ã‚‰é¸æŠã§ãã¾ã™
    - è¤‡æ•°ã®æ¡ä»¶ã‚’çµ„ã¿åˆã‚ã›ã¦ã‚ˆã‚Šçµã‚Šè¾¼ã‚“ã æ¤œç´¢ãŒå¯èƒ½ã§ã™
    
    Args:
        keyword: ã‚¿ã‚¤ãƒˆãƒ«ã€æ¦‚è¦ã€è‘—è€…åãªã©ã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
        source: ã‚½ãƒ¼ã‚¹ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆ"arxiv" ã¾ãŸã¯ "semantic_scholar"ï¼‰
        limit: è¡¨ç¤ºã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
        sort_by: ã‚½ãƒ¼ãƒˆåŸºæº–ï¼ˆ"date"=æ—¥ä»˜é †, "citations"=å¼•ç”¨æ•°é †, "title"=ã‚¿ã‚¤ãƒˆãƒ«é †ï¼‰
        sort_order: ã‚½ãƒ¼ãƒˆæ–¹å‘ï¼ˆ"asc"=æ˜‡é †, "desc"=é™é †ï¼‰
        filter_has_fulltext: Trueã®å ´åˆã€å…¨æ–‡ãŒåˆ©ç”¨å¯èƒ½ãªè«–æ–‡ã®ã¿ã‚’è¡¨ç¤º
        min_citations: æœ€å°å¼•ç”¨æ•°ï¼ˆã“ã‚Œã‚ˆã‚Šå°‘ãªã„å¼•ç”¨æ•°ã®è«–æ–‡ã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ï¼‰
        format: è¡¨ç¤ºå½¢å¼ï¼ˆ"detailed"=è©³ç´°, "compact"=ç°¡æ½”, "csv"=CSVå½¢å¼ï¼‰
        venue: ç‰¹å®šã®æ²è¼‰å…ˆï¼ˆã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ã‚„ä¼šè­°ï¼‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
        date_from: ã“ã®æ—¥ä»˜ä»¥é™ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
        date_to: ã“ã®æ—¥ä»˜ä»¥å‰ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
    
    Returns:
        æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ã«åˆè‡´ã™ã‚‹ä¿å­˜æ¸ˆã¿è«–æ–‡ã®ä¸€è¦§
    
    ä¾‹:
        * list_saved_papers(keyword="transform") - ã€Œtransformã€ã‚’å«ã‚€ã™ã¹ã¦ã®è«–æ–‡ã‚’è¡¨ç¤ºï¼ˆã€Œtransformerã€ã€Œtransformersã€ã€Œtransformationã€ãªã©ï¼‰
        * list_saved_papers(keyword="bert", sort_by="citations", sort_order="desc") - ã€Œbertã€ã‚’å«ã‚€è«–æ–‡ã‚’å¼•ç”¨æ•°ã®å¤šã„é †ã«è¡¨ç¤ºï¼ˆã€ŒBERTã€ã€ŒRoBERTaã€ã€ŒALBERTã€ã€ŒDistilBERTã€ãªã©ï¼‰
        * list_saved_papers(venue="NeurIPS") - NeurIPSä¼šè­°ã§ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ã‚’è¡¨ç¤ºï¼ˆã€ŒNeurIPS 2022ã€ã€ŒNeurIPS Workshopã€ãªã©ã‚‚å«ã‚€ï¼‰
        * list_saved_papers(source="arxiv", sort_by="citations", date_from="2023-01-01")
        * list_saved_papers(filter_has_fulltext=True, format="compact")
    """
    try:
        log_info(f"ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã‚’ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: '{keyword}', ã‚½ãƒ¼ãƒˆ: {sort_by} {sort_order}")
        
        # Get papers based on search criteria
        papers = paper_db.get_papers(
            keyword, source, limit, sort_by, sort_order,
            filter_has_fulltext, min_citations, venue, date_from, date_to
        )
        
        if not papers:
            return "æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ã«åˆè‡´ã™ã‚‹è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # Format results based on requested format
        if format == "csv":
            return _format_papers_as_csv(papers)
        elif format == "compact":
            return _format_papers_as_compact(papers)
        else:
            return _format_papers_as_detailed(papers)
            
    except Exception as e:
        log_error(f"ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã®ä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã®ä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

def _format_papers_as_csv(papers: List[Dict[str, Any]]) -> str:
    """
    è«–æ–‡ãƒªã‚¹ãƒˆã‚’CSVå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯è«–æ–‡ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’CSVï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šå€¤ï¼‰å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚
    CSVå½¢å¼ã¯è¡¨è¨ˆç®—ã‚½ãƒ•ãƒˆã§ã®åˆ†æã‚„ä»–ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ„ãƒ¼ãƒ«ã¨ã®é€£æºã«é©ã—ã¦ã„ã¾ã™ã€‚
    å‡ºåŠ›ã«ã¯ãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒå«ã¾ã‚Œã€å„è«–æ–‡ã®åŸºæœ¬çš„ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå«ã¾ã‚Œã¾ã™ã€‚
    å¼•ç”¨ç¬¦ã§å›²ã¾ã‚ŒãŸå€¤ã¯CSVã®æ¨™æº–çš„ãªã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã„ã¾ã™ã€‚
    
    å«ã¾ã‚Œã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰:
    - paper_id: è«–æ–‡ã®ä¸€æ„ã®è­˜åˆ¥å­
    - title: è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«
    - authors: è‘—è€…ãƒªã‚¹ãƒˆï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰
    - source: è«–æ–‡ã®ã‚½ãƒ¼ã‚¹ï¼ˆ"arXiv"ã¾ãŸã¯"Semantic Scholar"ï¼‰
    - url: è«–æ–‡ã®URL
    - citation_count: å¼•ç”¨æ•°
    - venue: æ²è¼‰å…ˆï¼ˆã‚¸ãƒ£ãƒ¼ãƒŠãƒ«åã‚„ä¼šè­°åï¼‰
    - published_date: ç™ºè¡¨æ—¥
    - collected_date: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åé›†ã•ã‚ŒãŸæ—¥ä»˜
    - full_text_available: å…¨æ–‡ãŒåˆ©ç”¨å¯èƒ½ã‹ã©ã†ã‹ã‚’ç¤ºã™ãƒ•ãƒ©ã‚°ï¼ˆ1=åˆ©ç”¨å¯èƒ½ã€0=åˆ©ç”¨ä¸å¯ï¼‰
    
    Args:
        papers: è«–æ–‡ã®ãƒªã‚¹ãƒˆã€‚å„è«–æ–‡ã¯è¾æ›¸å½¢å¼ã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€
        
    Returns:
        CSVå½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸè«–æ–‡ãƒ‡ãƒ¼ã‚¿
    """
    result = "paper_id,title,authors,source,url,citation_count,venue,published_date,collected_date,full_text_available\n"
    for paper in papers:
        result += f'"{paper["paper_id"]}","{paper["title"].replace("\"", "\"\"")}","{paper["authors"].replace("\"", "\"\"")}","{paper["source"]}","{paper["url"]}","{paper["citation_count"]}","{paper.get("venue", "").replace("\"", "\"\"")}","{paper["published_date"]}","{paper["collected_date"]}","{paper["full_text_available"]}"\n'
    return result

def _format_papers_as_compact(papers: List[Dict[str, Any]]) -> str:
    """
    è«–æ–‡ãƒªã‚¹ãƒˆã‚’ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯è«–æ–‡ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç°¡æ½”ãªå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™ã€‚å„è«–æ–‡ã®æœ€ã‚‚é‡è¦ãª
    æƒ…å ±ã ã‘ã‚’å«ã¿ã€å¤šæ•°ã®è«–æ–‡ã‚’ä¸€åº¦ã«é–²è¦§ã™ã‚‹ã®ã«é©ã—ã¦ã„ã¾ã™ã€‚ã‚¿ã‚¤ãƒˆãƒ«ã€è‘—è€…ï¼ˆçŸ­ç¸®è¡¨ç¤ºï¼‰,
    ç™ºè¡¨æ—¥ã€åé›†æ—¥ã€ã‚½ãƒ¼ã‚¹æƒ…å ±ãŒå«ã¾ã‚Œã€å…¨æ–‡åˆ©ç”¨å¯èƒ½ãªè«–æ–‡ã«ã¯ç‰¹åˆ¥ãªã‚¢ã‚¤ã‚³ãƒ³ï¼ˆğŸ“„ï¼‰ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
    ã¾ãŸã€å¼•ç”¨æ•°ãŒã‚ã‚‹å ´åˆã¯å¼•ç”¨æ•°ã‚‚è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
    
    è¡¨ç¤ºå½¢å¼:
    ```
    1. è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ« ğŸ“„ [å¼•ç”¨:123]
       è‘—è€…: è‘—è€…åãƒªã‚¹ãƒˆï¼ˆé•·ã„å ´åˆã¯çœç•¥ï¼‰
       ç™ºè¡¨: 2023-04-01 | åé›†: 2025-04-06
       ã‚½ãƒ¼ã‚¹: arXiv
    ```
    
    Args:
        papers: è«–æ–‡ã®ãƒªã‚¹ãƒˆã€‚å„è«–æ–‡ã¯è¾æ›¸å½¢å¼ã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€
        
    Returns:
        ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆå½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸè«–æ–‡ãƒªã‚¹ãƒˆ
    """
    result = f"åˆè¨ˆ {len(papers)} ä»¶ã®è«–æ–‡:\n\n"
    for i, paper in enumerate(papers, 1):
        full_text = "ğŸ“„" if paper["full_text_available"] else ""
        citations = f"[å¼•ç”¨:{paper['citation_count']}]" if paper['citation_count'] > 0 else ""
        result += f"{i}. {paper['title']} {full_text} {citations}\n"
        result += f"   è‘—è€…: {paper['authors'][:50]}{'...' if len(paper['authors']) > 50 else ''}\n"
        result += f"   ç™ºè¡¨: {paper['published_date']} | åé›†: {paper['collected_date']}\n"
        result += f"   ã‚½ãƒ¼ã‚¹: {paper['source']}\n\n"
    return result

def _format_papers_as_detailed(papers: List[Dict[str, Any]]) -> str:
    """
    è«–æ–‡ãƒªã‚¹ãƒˆã‚’è©³ç´°å½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯è«–æ–‡ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è©³ç´°ãªå½¢å¼ã§è¡¨ç¤ºã—ã¾ã™ã€‚å„è«–æ–‡ã«ã¤ã„ã¦åˆ©ç”¨å¯èƒ½ãª
    å…¨ã¦ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¡¨ç¤ºã—ã€å…¨æ–‡ãŒåˆ©ç”¨å¯èƒ½ã‹ã©ã†ã‹ã®æƒ…å ±ã‚‚å«ã¿ã¾ã™ã€‚
    æ¦‚è¦ï¼ˆabstractï¼‰ã¯é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã€æœ€åˆã®200æ–‡å­—ã®ã¿ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
    å„è«–æ–‡ã‚¨ãƒ³ãƒˆãƒªã¯æ°´å¹³ç·šã§åŒºåˆ‡ã‚‰ã‚Œã€èª­ã¿ã‚„ã™ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ãªã£ã¦ã„ã¾ã™ã€‚
    
    è¡¨ç¤ºã•ã‚Œã‚‹æƒ…å ±:
    - ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆå…¨æ–‡åˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ãã®æ—¨ã‚‚è¡¨ç¤ºï¼‰
    - è‘—è€…ãƒªã‚¹ãƒˆ
    - ã‚½ãƒ¼ã‚¹ï¼ˆarXivã¾ãŸã¯Semantic Scholarï¼‰
    - URL
    - å¼•ç”¨æ•°ï¼ˆã‚ã‚‹å ´åˆï¼‰
    - æ²è¼‰å…ˆï¼ˆã‚ã‚‹å ´åˆï¼‰
    - ç™ºè¡¨æ—¥
    - æ¦‚è¦ï¼ˆæœ€åˆã®200æ–‡å­—ã€é•·ã„å ´åˆã¯çœç•¥ï¼‰
    - åé›†æ—¥
    - è«–æ–‡ID
    
    Args:
        papers: è«–æ–‡ã®ãƒªã‚¹ãƒˆã€‚å„è«–æ–‡ã¯è¾æ›¸å½¢å¼ã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€
        
    Returns:
        è©³ç´°å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸè«–æ–‡ãƒªã‚¹ãƒˆ
    """
    result = f"åˆè¨ˆ {len(papers)} ä»¶ã®è«–æ–‡:\n\n"
    
    for paper in papers:
        full_text = "ï¼ˆå…¨æ–‡ã‚ã‚Šï¼‰" if paper["full_text_available"] else ""
        result += f"ã‚¿ã‚¤ãƒˆãƒ«: {paper['title']} {full_text}\n"
        result += f"è‘—è€…: {paper['authors']}\n"
        result += f"ã‚½ãƒ¼ã‚¹: {paper['source']}\n"
        result += f"URL: {paper['url']}\n"
        
        if paper["citation_count"] > 0:
            result += f"å¼•ç”¨æ•°: {paper['citation_count']}\n"
        
        if paper.get("venue"):
            result += f"æ²è¼‰å…ˆ: {paper['venue']}\n"
        
        # ç™ºè¡¨æ—¥ã‚’è¿½åŠ 
        if paper.get("published_date"):
            result += f"ç™ºè¡¨æ—¥: {paper['published_date']}\n"
        
        # æ¦‚è¦ã¯é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if paper.get("abstract"):
            abstract_preview = paper["abstract"][:200].replace("\n", " ")
            result += f"æ¦‚è¦: {abstract_preview}{'...' if len(paper['abstract']) > 200 else ''}\n"
        
        result += f"åé›†æ—¥: {paper['collected_date']}\n"
        result += f"ID: {paper['paper_id']}\n"
        result += "-" * 50 + "\n"
        
    return result

@mcp.tool()
async def rank_papers_by_citations(keyword: str = "", limit: int = 10) -> str:
    """
    ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã‚’å¼•ç”¨æ•°ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã‚’å¼•ç”¨æ•°ãŒå¤šã„é †ã«ã‚½ãƒ¼ãƒˆã—ã€æœ€ã‚‚å½±éŸ¿åŠ›ã®é«˜ã„è«–æ–‡ã‚’
    è¦‹ã¤ã‘ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ç‰¹å®šã®ç ”ç©¶åˆ†é‡ã«ãŠã‘ã‚‹é‡è¦è«–æ–‡ã‚’ç‰¹å®šã—ãŸã‚Šã€
    ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æŠŠæ¡ã™ã‚‹ã®ã«ä¾¿åˆ©ã§ã™ã€‚
    
    åˆ©ç”¨ã®ãƒ’ãƒ³ãƒˆ:
    - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è«–æ–‡ã®ã¿ãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°ã•ã‚Œã¾ã™
    - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã€è‘—è€…åã€æ¦‚è¦ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§æ¤œç´¢ã•ã‚Œã¾ã™
    - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç©ºã«ã™ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®ã™ã¹ã¦ã®è«–æ–‡ãŒå¯¾è±¡ã«ãªã‚Šã¾ã™
    - å¼•ç”¨æ•°ãŒåŒã˜è«–æ–‡ã¯åé›†æ—¥é †ï¼ˆæ–°ã—ã„ã‚‚ã®ãŒå…ˆï¼‰ã«è¡¨ç¤ºã•ã‚Œã¾ã™
    
    Args:
        keyword: ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰ã€‚ç©ºã®å ´åˆã¯å…¨è«–æ–‡ãŒå¯¾è±¡
        limit: è¡¨ç¤ºã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
    
    Returns:
        å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‚å„è«–æ–‡ã®è©³ç´°æƒ…å ±ã‚’å«ã¿ã¾ã™
    
    ä¾‹:
        * rank_papers_by_citations() - ã™ã¹ã¦ã®è«–æ–‡ã®å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        * rank_papers_by_citations("machine learning", 20) - æ©Ÿæ¢°å­¦ç¿’é–¢é€£è«–æ–‡ã®ãƒˆãƒƒãƒ—20
        * rank_papers_by_citations("neural networks") - ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é–¢é€£è«–æ–‡ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    """
    try:
        log_info(f"è«–æ–‡ã®å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: '{keyword}'")
        papers = paper_db.get_papers(
            keyword=keyword,
            limit=limit,
            sort_by="citations",
            sort_order="desc"
        )
        
        if not papers:
            return "æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ã«åˆè‡´ã™ã‚‹è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        result = f"å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆ{keyword if keyword else 'å…¨ã¦'}ï¼‰:\n\n"
        
        for i, paper in enumerate(papers, 1):
            result += f"{i}. {paper['title']}\n"
            result += f"   è‘—è€…: {paper['authors']}\n"
            result += f"   å¼•ç”¨æ•°: {paper['citation_count']}\n"
            if paper['venue']:
                result += f"   æ²è¼‰å…ˆ: {paper['venue']}\n"
            result += f"   ã‚½ãƒ¼ã‚¹: {paper['source']}\n"
            result += f"   URL: {paper['url']}\n"
            result += f"   åé›†æ—¥: {paper['collected_date']}\n"
            result += "-" * 50 + "\n"
        
        return result
    except Exception as e:
        log_error(f"å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def list_papers_by_venue(venue: str = "", limit: int = 10) -> str:
    """
    ç‰¹å®šã®æ²è¼‰å…ˆï¼ˆã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ã‚„ä¼šè­°ï¼‰ã®è«–æ–‡ä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯ç‰¹å®šã®ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ã‚„å­¦è¡“ä¼šè­°ã€ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ãªã©ã«æ²è¼‰ã•ã‚ŒãŸè«–æ–‡ã‚’
    ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚æ²è¼‰å…ˆåˆ¥ã«è«–æ–‡ã‚’æ•´ç†ã™ã‚‹ã“ã¨ã§ã€ç‰¹å®šã®åˆ†é‡ã‚„
    å‡ºç‰ˆç‰©ã«ãŠã‘ã‚‹ç ”ç©¶å‹•å‘ã‚’æŠŠæ¡ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
    
    åˆ©ç”¨ã®ãƒ’ãƒ³ãƒˆ:
    - venueãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯å®Œå…¨ä¸€è‡´ã§ãªãéƒ¨åˆ†ä¸€è‡´æ¤œç´¢ãŒä½¿ç”¨ã•ã‚Œã¾ã™
    - ç©ºã®venueã‚’æŒ‡å®šã™ã‚‹ã¨ã€æ²è¼‰å…ˆæƒ…å ±ãŒã‚ã‚‹å…¨ã¦ã®è«–æ–‡ãŒå¯¾è±¡ã«ãªã‚Šã¾ã™
    - è¤‡æ•°ã®æ²è¼‰å…ˆã«ãƒãƒƒãƒã™ã‚‹å ´åˆã¯ã€ãã‚Œãã‚Œã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã¦è¡¨ç¤ºã•ã‚Œã¾ã™
    - å„ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®è«–æ–‡ã¯å¼•ç”¨æ•°ã®å¤šã„é †ã«ã‚½ãƒ¼ãƒˆã•ã‚Œã¾ã™
    
    Args:
        venue: æ²è¼‰å…ˆã®åå‰ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰ã€‚ä¾‹: "CVPR", "Nature", "ACL"ãªã©
        limit: è¡¨ç¤ºã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
    
    Returns:
        æ²è¼‰å…ˆåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã•ã‚ŒãŸè«–æ–‡ä¸€è¦§
    
    ä¾‹:
        * list_papers_by_venue("NIPS") - NIPS/NeurIPSä¼šè­°ã®è«–æ–‡ã‚’è¡¨ç¤º
        * list_papers_by_venue("Journal") - "Journal"ã‚’å«ã‚€å…¨ã¦ã®æ²è¼‰å…ˆã®è«–æ–‡ã‚’è¡¨ç¤º
        * list_papers_by_venue() - å…¨ã¦ã®æ²è¼‰å…ˆã®è«–æ–‡ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦è¡¨ç¤º
    """
    try:
        log_info(f"æ²è¼‰å…ˆã§ã®è«–æ–‡æ¤œç´¢: '{venue}'")
        papers = paper_db.get_papers_by_venue(venue, limit)
        
        if not papers:
            return "æŒ‡å®šã•ã‚ŒãŸæ²è¼‰å…ˆã®è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        venues = {}
        for paper in papers:
            v = paper['venue']
            if v not in venues:
                venues[v] = []
            venues[v].append(paper)
        
        result = f"æ²è¼‰å…ˆåˆ¥è«–æ–‡ä¸€è¦§ï¼ˆ{venue if venue else 'å…¨ã¦'}ï¼‰:\n\n"
        
        for v, v_papers in venues.items():
            result += f"ã€{v}ã€‘- {len(v_papers)}ä»¶\n"
            for paper in v_papers:
                result += f"- {paper['title']}\n"
                result += f"  è‘—è€…: {paper['authors']}\n"
                result += f"  å¼•ç”¨æ•°: {paper['citation_count']}\n"
                result += f"  URL: {paper['url']}\n"
            result += "-" * 50 + "\n"
        
        return result
    except Exception as e:
        log_error(f"æ²è¼‰å…ˆåˆ¥è«–æ–‡ä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"æ²è¼‰å…ˆåˆ¥è«–æ–‡ä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

# æ®‹ã‚Šã®é–¢æ•°ã¯åŒæ§˜ã«ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™
@mcp.tool()
async def list_top_venues(limit: int = 10) -> str:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹è«–æ–‡ã®ãƒˆãƒƒãƒ—æ²è¼‰å…ˆä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯ä¿å­˜ã•ã‚Œã¦ã„ã‚‹è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„æ²è¼‰å…ˆï¼ˆã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ã‚„ä¼šè­°ï¼‰ã®
    çµ±è¨ˆæƒ…å ±ã‚’é›†è¨ˆã—ã€å¹³å‡å¼•ç”¨æ•°ã®å¤šã„é †ã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã—ã¾ã™ã€‚ç ”ç©¶åˆ†é‡ã«ãŠã‘ã‚‹
    ä¸»è¦ãªæ²è¼‰å…ˆã‚’æŠŠæ¡ã—ãŸã‚Šã€å½±éŸ¿åŠ›ã®é«˜ã„ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ã‚’ç‰¹å®šã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚
    
    é›†è¨ˆæƒ…å ±ã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¾ã™ï¼š
    - æ²è¼‰å…ˆå
    - ãã®æ²è¼‰å…ˆã®è«–æ–‡æ•°
    - å¹³å‡å¼•ç”¨æ•°
    - æœ€å¤§å¼•ç”¨æ•°
    
    çµæœã¯å¹³å‡å¼•ç”¨æ•°ã®å¤šã„é †ã«ã‚½ãƒ¼ãƒˆã•ã‚Œã¾ã™ã€‚
    
    Args:
        limit: è¡¨ç¤ºã™ã‚‹æ²è¼‰å…ˆã®æœ€å¤§æ•°
    
    Returns:
        å¹³å‡å¼•ç”¨æ•°ã®å¤šã„é †ã«ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸæ²è¼‰å…ˆä¸€è¦§
    
    ä¾‹:
        * list_top_venues() - ãƒˆãƒƒãƒ—10ã®æ²è¼‰å…ˆã‚’è¡¨ç¤º
        * list_top_venues(20) - ãƒˆãƒƒãƒ—20ã®æ²è¼‰å…ˆã‚’è¡¨ç¤º
    """
    try:
        log_info(f"ãƒˆãƒƒãƒ—æ²è¼‰å…ˆä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ä¸Šä½ {limit} ä»¶")
        venues = paper_db.get_top_venues(limit)
        
        if not venues:
            return "æ²è¼‰å…ˆæƒ…å ±ãŒã‚ã‚‹è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        result = "ãƒˆãƒƒãƒ—æ²è¼‰å…ˆä¸€è¦§ï¼ˆå¹³å‡å¼•ç”¨æ•°é †ï¼‰:\n\n"
        
        for i, (venue, paper_count, avg_citations, max_citations) in enumerate(venues, 1):
            result += f"{i}. {venue}\n"
            result += f"   è«–æ–‡æ•°: {paper_count}ä»¶\n"
            result += f"   å¹³å‡å¼•ç”¨æ•°: {avg_citations:.1f}\n"
            result += f"   æœ€å¤§å¼•ç”¨æ•°: {max_citations}\n"
            result += "-" * 50 + "\n"
        
        return result
    except Exception as e:
        log_error(f"ãƒˆãƒƒãƒ—æ²è¼‰å…ˆä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"ãƒˆãƒƒãƒ—æ²è¼‰å…ˆä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def get_paper_details(paper_id: str) -> str:
    """
    ç‰¹å®šã®è«–æ–‡ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚paper_idã¾ãŸã¯è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢ã§ãã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯è«–æ–‡ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å®Œå…¨ãªå½¢ã§è¡¨ç¤ºã—ã¾ã™ã€‚è«–æ–‡ã®IDã€ã‚¿ã‚¤ãƒˆãƒ«ã€è‘—è€…ã€
    æ¦‚è¦ã€URLã€æ²è¼‰å…ˆã€å¼•ç”¨æ•°ãªã©ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹å…¨ã¦ã®æƒ…å ±ãŒ
    å«ã¾ã‚Œã¾ã™ã€‚
    
    æ¤œç´¢ã®ãƒ’ãƒ³ãƒˆ:
    - å®Œå…¨ãªPaper IDã§ã®æ¤œç´¢ãŒæœ€ã‚‚æ­£ç¢ºã§ã™ (ä¾‹: "1703.05190v3")
    - ã‚¿ã‚¤ãƒˆãƒ«ã§ã®æ¤œç´¢ã‚‚å¯èƒ½ã§ã€éƒ¨åˆ†ä¸€è‡´ã§æ¤œç´¢ã•ã‚Œã¾ã™ (ä¾‹: "Transformer" ã‚„ "Attention")
    - ã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢ã®å ´åˆã€æœ€åˆã«ãƒãƒƒãƒã—ãŸè«–æ–‡ã®ã¿ãŒè¡¨ç¤ºã•ã‚Œã¾ã™
    - æ¤œç´¢æ–‡å­—åˆ—ã¯å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ã¾ã›ã‚“
    - PDFãŒåˆ©ç”¨å¯èƒ½ãªè«–æ–‡ã¯ã€Œå…¨æ–‡ã‚ã‚Šã€ã¨è¡¨ç¤ºã•ã‚Œã€get_paper_full_text()ã§å…¨æ–‡ã‚’ç¢ºèªã§ãã¾ã™
    - list_saved_papers()ã§æ¤œç´¢ã—ãŸçµæœã‹ã‚‰ã€èˆˆå‘³ã®ã‚ã‚‹è«–æ–‡ã®IDã‚„ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä½¿ã†ã¨ã‚¹ãƒ ãƒ¼ã‚ºã§ã™
    
    Args:
        paper_id: è«–æ–‡ã®ID (ä¾‹: "1703.05190v3") ã¾ãŸã¯ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸€éƒ¨ (ä¾‹: "Attention is All You Need")
    
    Returns:
        è«–æ–‡ã®è©³ç´°æƒ…å ±ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€è‘—è€…ã€æ¦‚è¦ã€URLã€å¼•ç”¨æ•°ãªã©ï¼‰
    
    ä¾‹:
        * get_paper_details("1703.05190v3") - arXiv IDã«ã‚ˆã‚‹æ¤œç´¢
        * get_paper_details("Attention is All You Need") - æœ‰åãªTransformerè«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢
        * get_paper_details("BERT") - BERTã«é–¢ã™ã‚‹è«–æ–‡ã‚’æ¤œç´¢ï¼ˆæœ€åˆã«ãƒãƒƒãƒã—ãŸè«–æ–‡ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼‰
    """
    try:
        log_info(f"è«–æ–‡è©³ç´°ã‚’æ¤œç´¢: '{paper_id}'")
        paper = paper_db.get_paper_by_id(paper_id)
        
        if not paper:
            return f"ID ã¾ãŸã¯ ã‚¿ã‚¤ãƒˆãƒ« '{paper_id}' ã®è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        full_text = "ï¼ˆå…¨æ–‡ã‚ã‚Šï¼‰" if paper["full_text_available"] else "ï¼ˆå…¨æ–‡ãªã—ï¼‰"
        result = f"ã‚¿ã‚¤ãƒˆãƒ«: {paper['title']} {full_text}\n"
        result += f"è‘—è€…: {paper['authors']}\n"
        result += f"å‡ºç‰ˆå¹´: {paper['published_date']}\n"
        result += f"ã‚½ãƒ¼ã‚¹: {paper['source']}\n"
        result += f"URL: {paper['url']}\n"
        
        if paper["citation_count"] > 0:
            result += f"å¼•ç”¨æ•°: {paper['citation_count']}\n"
        
        if paper["venue"]:
            result += f"æ²è¼‰å…ˆ: {paper['venue']}\n"
        
        if paper["full_text_available"] and paper["pdf_path"]:
            result += f"PDF: {paper['pdf_path']}\n"
        
        result += f"åé›†æ—¥: {paper['collected_date']}\n"
        result += f"\næ¦‚è¦:\n{paper['abstract']}\n"
        
        if paper['keywords']:
            result += f"\nã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {paper['keywords']}\n"
        
        return result
    except Exception as e:
        log_error(f"è«–æ–‡è©³ç´°è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"è«–æ–‡è©³ç´°è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def get_paper_full_text(paper_id: str, max_length: int = 1000000) -> str:
    """
    ä¿å­˜ã•ã‚ŒãŸPDFã‹ã‚‰è«–æ–‡ã®å…¨æ–‡ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯PDFã‹ã‚‰æŠ½å‡ºã—ãŸè«–æ–‡ã®å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤ºã—ã¾ã™ã€‚æœ€åˆã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®
    å…¨æ–‡ã‚’ç¢ºèªã—ã€ãªã‘ã‚Œã°PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’è©¦ã¿ã¾ã™ã€‚æŠ½å‡ºã•ã‚ŒãŸå…¨æ–‡ã¯
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã€æ¬¡å›ã‹ã‚‰ç´ æ—©ãã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
    
    ä½¿ç”¨ä¸Šã®æ³¨æ„:
    - paper_idã«ã¯IDã ã‘ã§ãªãè«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ã‚‚æŒ‡å®šå¯èƒ½ã§ã™ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
    - PDFå†…ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å“è³ªã«ã‚ˆã£ã¦ã€æŠ½å‡ºçµæœã®å“è³ªãŒå¤‰ã‚ã‚Šã¾ã™
    - æ•°å¼ã‚„å›³è¡¨ã€ç‰¹æ®Šè¨˜å·ãªã©ã¯æ­£ã—ãæŠ½å‡ºã•ã‚Œãªã„å ´åˆãŒã‚ã‚Šã¾ã™
    - é•·ã„è«–æ–‡ã¯è‡ªå‹•çš„ã«max_lengthãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æŒ‡å®šã—ãŸé•·ã•ã«åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã™
    - å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹å ´åˆã¯ã€max_lengthãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é©åˆ‡ã«åˆ¶é™ã™ã‚‹ã¨ä¾¿åˆ©ã§ã™
    - è«–æ–‡PDFãŒãªã„å ´åˆã‚„æŠ½å‡ºã«å¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã¾ã™
    
    æ´»ç”¨æ–¹æ³•:
    - ç ”ç©¶å†…å®¹ã®è©³ç´°ç¢ºèªï¼šsearch_full_textã§è¦‹ã¤ã‘ãŸç®‡æ‰€ã®å‰å¾Œæ–‡è„ˆã‚’ã‚ˆã‚Šåºƒãèª­ã‚€
    - ç‰¹å®šã®ç« ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ç¢ºèªï¼šé€šå¸¸ã€è«–æ–‡PDFã®ç›®æ¬¡ã‹ã‚‰èˆˆå‘³ã®ã‚ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¦‹ã¤ã‘ã¦èª­ã‚€
    - å‚è€ƒæ–‡çŒ®ãƒªã‚¹ãƒˆã®ç¢ºèªï¼šå¤šãã®è«–æ–‡ã§ã¯æœ«å°¾ã«å‚è€ƒæ–‡çŒ®ãƒªã‚¹ãƒˆãŒã‚ã‚Šã€é–¢é€£ç ”ç©¶ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã‚‹
    - æ‰‹æ³•ã‚„å®Ÿé¨“ã®è©³ç´°ç¢ºèªï¼šè«–æ–‡ã®æ¦‚è¦ã ã‘ã§ãªãã€å…·ä½“çš„ãªå®Ÿè£…ã‚„å®Ÿé¨“æ¡ä»¶ã‚’èª¿ã¹ã‚‹
    
    Args:
        paper_id: è«–æ–‡ã®IDã¾ãŸã¯ã‚¿ã‚¤ãƒˆãƒ«ã€‚å®Œå…¨ãªIDã¾ãŸã¯ã‚¿ã‚¤ãƒˆãƒ«ã®éƒ¨åˆ†æ–‡å­—åˆ—ã‚’æŒ‡å®šã§ãã¾ã™
        max_length: è¿”ã™å…¨æ–‡ã®æœ€å¤§é•·ã•ï¼ˆæ–‡å­—æ•°ï¼‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯100ä¸‡æ–‡å­—ã§ã€ã“ã‚Œã‚’è¶…ãˆã‚‹éƒ¨åˆ†ã¯åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã™
    
    Returns:
        è«–æ–‡ã®å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã€‚é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‰ã‚ŒãŸå†…å®¹
    
    ä¾‹:
        * get_paper_full_text("1703.05190v3") - Attention is All You Needã®å…¨æ–‡ã‚’å–å¾—
        * get_paper_full_text("BERT", 30000) - BERTã«é–¢ã™ã‚‹è«–æ–‡ã®å…¨æ–‡ã‚’å–å¾—ï¼ˆæœ€å¤§3ä¸‡æ–‡å­—ï¼‰
        * get_paper_full_text("Transformer architecture") - Transformerã«ã¤ã„ã¦ã®è«–æ–‡ã®å…¨æ–‡ã‚’å–å¾—
    """
    try:
        log_info(f"è«–æ–‡å…¨æ–‡ã‚’å–å¾—: '{paper_id}'")
        paper = paper_db.get_paper_by_id(paper_id)
        
        if not paper:
            return f"ID ã¾ãŸã¯ ã‚¿ã‚¤ãƒˆãƒ« '{paper_id}' ã®è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        if not paper["full_text_available"] or not paper["pdf_path"]:
            return f"è«–æ–‡ '{paper['title']}' ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        full_text = paper["full_text"] if paper["full_text"] is not None else None
        
        if not full_text:
            log_info(f"PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º: {paper['pdf_path']}")
            full_text = extract_text_from_pdf(paper["pdf_path"])
            
            if full_text:
                paper_db.save_full_text(paper["paper_id"], full_text)
            else:
                return f"è«–æ–‡ '{paper['title']}' ã®PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    
        if len(full_text) > max_length:
            full_text = full_text[:max_length] + f"\n\n... (ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã„ãŸã‚åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸã€‚å…¨æ–‡ã¯ {len(full_text)} æ–‡å­—ã‚ã‚Šã¾ã™)"
    
        return f"ã‚¿ã‚¤ãƒˆãƒ«: {paper['title']}\nè‘—è€…: {paper['authors']}\n\nå…¨æ–‡:\n{full_text}"
    except Exception as e:
        log_error(f"è«–æ–‡å…¨æ–‡å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"è«–æ–‡å…¨æ–‡å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def search_full_text(query: str, limit: int = 5) -> str:
    """
    è«–æ–‡ã®å…¨æ–‡ã‹ã‚‰ç‰¹å®šã®æ–‡å­—åˆ—ã‚’æ¤œç´¢ã—ã¾ã™ã€‚PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸå†…å®¹ã‚’æ¤œç´¢å¯¾è±¡ã¨ã—ã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚ŒãŸPDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸè«–æ–‡å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰
    æŒ‡å®šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æ¤œç´¢ã—ã¾ã™ã€‚æ¤œç´¢çµæœã«ã¯å„è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã€è‘—è€…ã¨
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€å‰å¾Œã®æ–‡è„ˆï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ãŒè¡¨ç¤ºã•ã‚Œã‚‹ãŸã‚ã€ç ”ç©¶æ‰‹æ³•ã‚„æ¦‚å¿µã€
    ç”¨èªãªã©ãŒã©ã®ã‚ˆã†ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ã‚’ã™ãã«ç¢ºèªã§ãã¾ã™ã€‚
    
    ãƒ­ãƒ¼ã‚«ãƒ«å…¨æ–‡æ¤œç´¢ã®ç‰¹å¾´ã¨æ´»ç”¨æ³•:
    - PDFãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹è«–æ–‡ã®ã¿ãŒæ¤œç´¢å¯¾è±¡ã¨ãªã‚Šã¾ã™
    - åˆå›æ¤œç´¢æ™‚ã«PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã™ã‚‹ãŸã‚é«˜é€Ÿã«æ¤œç´¢ã§ãã¾ã™
    - æ•°å¼ã‚„å›³è¡¨ã®è¨˜è¿°ã‚’æ¢ã—ãŸã‚Šã€ç‰¹å®šã®æ‰‹æ³•ã«ã¤ã„ã¦è©³ã—ãæ›¸ã‹ã‚Œã¦ã„ã‚‹è«–æ–‡ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã¾ã™
    - å˜èªã ã‘ã§ãªãã€ãƒ•ãƒ¬ãƒ¼ã‚ºã‚„è¤‡æ•°å˜èªã‚’çµ„ã¿åˆã‚ã›ãŸæ¤œç´¢ã‚‚å¯èƒ½ã§ã™
    - å¤§æ–‡å­—å°æ–‡å­—ã¯åŒºåˆ¥ã•ã‚Œãªã„ãŸã‚ã€"BERT"ã‚‚"Bert"ã‚‚"bert"ã‚‚åŒã˜çµæœã«ãªã‚Šã¾ã™
    - ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰æ¤œç´¢ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ï¼ˆã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ã€Œ*ã€ã‚’ä½¿ç”¨ï¼‰
    
    ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰æ¤œç´¢ã®å…·ä½“ä¾‹:
    1. å‰æ–¹ä¸€è‡´æ¤œç´¢ (ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹æ¤œç´¢):
       * "transform*" â†’ "transform", "transformer", "transformers", "transformation" ãªã©
       * "neural*" â†’ "neural", "neurons", "neural networks" ãªã©
       * "GAN*" â†’ "GAN", "GANs", "GAN-based" ãªã©
    
    2. å¾Œæ–¹ä¸€è‡´æ¤œç´¢ (ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹æ¤œç´¢):
       * "*former" â†’ "transformer", "performer", "conformer" ãªã©
       * "*embedding" â†’ "word embedding", "token embedding" ãªã©
       * "*learning" â†’ "deep learning", "machine learning", "reinforcement learning" ãªã©
    
    3. ä¸­é–“ä¸€è‡´æ¤œç´¢ (ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ä¸¡ç«¯):
       * "*embed*" â†’ "embedding", "embedded", "embeddings" ãªã©
       * "*attent*" â†’ "attention", "self-attention", "multi-head attention" ãªã©
       * "*bert*" â†’ "BERT", "RoBERTa", "ALBERT", "DistilBERT" ãªã©
    
    4. è¤‡åˆãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰æ¤œç´¢:
       * "self*attention" â†’ "self-attention", "self attention" ãªã©
       * "conv*network" â†’ "convolutional network", "convolution network" ãªã©
       * "deep*learning" â†’ "deep learning", "deep reinforcement learning" ãªã©
    
    Args:
        query: æ¤œç´¢ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã¯åŒºåˆ¥ã•ã‚Œã¾ã›ã‚“ï¼‰ã€‚ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰æ–‡å­—ã€Œ*ã€ã‚‚ä½¿ç”¨å¯èƒ½
        limit: è¡¨ç¤ºã™ã‚‹æœ€å¤§ã®æ¤œç´¢çµæœæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
    
    Returns:
        æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆã€‚å„çµæœã«ã¯è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ã€è‘—è€…ã€æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€å‰å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç´„200æ–‡å­—ï¼‰ãŒå«ã¾ã‚Œã¾ã™
    
    ä¾‹:
        * search_full_text("attention mechanism") - ã€Œattention mechanismã€ã«ã¤ã„ã¦è¨€åŠã—ã¦ã„ã‚‹è«–æ–‡ã‚’æ¤œç´¢
        * search_full_text("transform*", 10) - "transform"ã§å§‹ã¾ã‚‹å˜èªã‚’å«ã‚€è«–æ–‡ã‚’æœ€å¤§10ä»¶æ¤œç´¢
        * search_full_text("*former") - "former"ã§çµ‚ã‚ã‚‹å˜èªï¼ˆtransformer, performerãªã©ï¼‰ã‚’å«ã‚€è«–æ–‡ã‚’æ¤œç´¢
        * search_full_text("*attent*") - "attent"ã‚’å«ã‚€å˜èªï¼ˆattention, self-attentionãªã©ï¼‰ã‚’å«ã‚€è«–æ–‡ã‚’æ¤œç´¢
        * search_full_text("*bert*") - BERTãŠã‚ˆã³ãã®æ´¾ç”Ÿãƒ¢ãƒ‡ãƒ«ï¼ˆRoBERTa, ALBERTãªã©ï¼‰ã«é–¢ã™ã‚‹è¨˜è¿°ã‚’æ¤œç´¢
    """
    try:
        log_info(f"è«–æ–‡å…¨æ–‡ã‚’æ¤œç´¢: '{query}'")
        papers = paper_db.get_papers(filter_has_fulltext=True, limit=100)  # ã‚ˆã‚Šå¤šãã®è«–æ–‡ã‚’å–å¾—ã—ã¦æ¤œç´¢
        
        if not papers:
            return "å…¨æ–‡ãŒåˆ©ç”¨å¯èƒ½ãªè«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        results = []
        
        for paper in papers:
            full_text = paper["full_text"] if paper["full_text"] is not None else None
            if not full_text and paper["pdf_path"]:
                full_text = extract_text_from_pdf(paper["pdf_path"])
                if full_text:
                    paper_db.save_full_text(paper["paper_id"], full_text)
            
            if full_text and query.lower() in full_text.lower():
                index = full_text.lower().find(query.lower())
                start = max(0, index - 100)
                end = min(len(full_text), index + len(query) + 100)
                
                context = full_text[start:end].replace("\n", " ")
                if start > 0:
                    context = "..." + context
                if end < len(full_text):
                    context = context + "..."
                
                results.append({
                    "paper_id": paper["paper_id"],
                    "title": paper["title"],
                    "authors": paper["authors"],
                    "context": context
                })
                
                if len(results) >= limit:
                    break
        
        if not results:
            return f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{query}' ã‚’å«ã‚€è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        result_text = f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{query}' ã‚’å«ã‚€è«–æ–‡: {len(results)}ä»¶\n\n"
        
        for i, res in enumerate(results, 1):
            result_text += f"{i}. ã‚¿ã‚¤ãƒˆãƒ«: {res['title']}\n"
            result_text += f"   è‘—è€…: {res['authors']}\n"
            result_text += f"   ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {res['context']}\n"
            result_text += f"   (Paper ID: {res['paper_id']})\n\n"
        
        return result_text
    except Exception as e:
        log_error(f"å…¨æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"å…¨æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def export_summaries(format: str = "json") -> str:
    """
    ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã®è¦ç´„ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹è«–æ–‡ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šã•ã‚ŒãŸå½¢å¼ã§
    ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ãŸã‚Šã€ä»–ã®ãƒ„ãƒ¼ãƒ«ã§
    åˆ†æã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã¾ã™ã€‚ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯è‡ªå‹•çš„ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã®
    åå‰ã§ä¿å­˜ã•ã‚Œã¾ã™ã€‚
    
    ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½ã®ç‰¹å¾´:
    - æœ€å¤§1000ä»¶ã®è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãŒã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚Œã¾ã™
    - ãƒ•ã‚¡ã‚¤ãƒ«åã«ã¯è‡ªå‹•çš„ã«æ—¥æ™‚ãŒä»˜åŠ ã•ã‚Œã¾ã™ (ä¾‹: paper_summaries_20250406_123045.json)
    - JSONå½¢å¼ã¯ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ€§ã‚’ä¿æŒã—ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ã®å†åˆ©ç”¨ã«æœ€é©ã§ã™
    - CSVå½¢å¼ã¯è¡¨è¨ˆç®—ã‚½ãƒ•ãƒˆãªã©ã§ã®é–²è¦§ãƒ»åˆ†æã«é©ã—ã¦ã„ã¾ã™
    
    Args:
        format: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼ ("json"=JSONå½¢å¼, "csv"=CSVå½¢å¼)
    
    Returns:
        ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆçµæœã®å ±å‘Šï¼ˆä¿å­˜å…ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å«ã‚€ï¼‰
    
    ä¾‹:
        * export_summaries() - JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        * export_summaries("csv") - CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    """
    try:
        log_info(f"è«–æ–‡è¦ç´„ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: å½¢å¼ = {format}")
        papers = paper_db.get_papers(limit=1000)  # æœ€å¤§1000ä»¶ã®è«–æ–‡ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        
        if not papers:
            return "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if format.lower() == "json":
            export_path = os.path.join(DATA_DIR, f"paper_summaries_{timestamp}.json")
            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(papers, f, ensure_ascii=False, indent=2)
        else:
            export_path = os.path.join(DATA_DIR, f"paper_summaries_{timestamp}.csv")
            df = pd.DataFrame(papers)
            df.to_csv(export_path, index=False)
        
        return f"è«–æ–‡è¦ç´„ã‚’ {export_path} ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚"
    except Exception as e:
        log_error(f"è¦ç´„ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"è¦ç´„ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def search_papers_by_date_range(
    query: str,
    start_date: str,
    end_date: str,
    source: str = "both",
    limit: int = 5
) -> str:
    """
    æŒ‡å®šã—ãŸæ—¥ä»˜ç¯„å›²å†…ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ã‚’æ¤œç´¢ã—ã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯ç‰¹å®šã®æœŸé–“å†…ã«å‡ºç‰ˆã•ã‚ŒãŸè«–æ–‡ã‚’æ¤œç´¢ã—ã¾ã™ã€‚ç ”ç©¶åˆ†é‡ã®æ™‚ç³»åˆ—çš„ãª
    ç™ºå±•ã‚’è¿½è·¡ã—ãŸã‚Šã€ç‰¹å®šã®æœŸé–“ã«ãŠã‘ã‚‹ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æŠŠæ¡ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚
    æœ€æ–°ã®ç ”ç©¶å‹•å‘ã‚’èª¿æŸ»ã™ã‚‹å ´åˆã‚„ã€æ­´å²çš„ãªè«–æ–‡ã‚’æ¢ã™å ´åˆã«ä¾¿åˆ©ã§ã™ã€‚
    
    æ—¥ä»˜ç¯„å›²æ¤œç´¢ã®ç‰¹å¾´:
    - æ—¥ä»˜ã¯å¿…ãšYYYY-MM-DDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆä¾‹: 2023-01-01ï¼‰
    - æ¤œç´¢ã¯æŒ‡å®šã—ãŸæ—¥ä»˜ã‚’å«ã‚€ç¯„å›²ã§è¡Œã‚ã‚Œã¾ã™ï¼ˆé–‹å§‹æ—¥ã¨çµ‚äº†æ—¥ã‚’å«ã‚€ï¼‰
    - æ—¥ä»˜ç¯„å›²ãŒåºƒã™ãã‚‹ã¨æ¤œç´¢çµæœãŒå¤šããªã‚Šã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
    - arXivã¨Semantic Scholarã§ã¯æ—¥ä»˜ã®ç²¾åº¦ã‚„åˆ©ç”¨å¯èƒ½ãªç¯„å›²ãŒç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™
    - arXivã§ã¯æ­£ç¢ºãªæ—¥ä»˜æ¤œç´¢ãŒå¯èƒ½ã§ã™ãŒã€Semantic Scholarã®å ´åˆã¯å¹´å˜ä½ã®æ¤œç´¢ã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™
    - æ¤œç´¢çµæœã¯è‡ªå‹•çš„ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã€å¾Œã§å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™
    
    Args:
        query: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã€‚arXivã§ã¯é«˜åº¦ãªæ¤œç´¢æ§‹æ–‡ã‚‚ã‚µãƒãƒ¼ãƒˆï¼ˆä¾‹: "machine AND learning"ï¼‰
        start_date: é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰ã€‚ã“ã®æ—¥ä»˜ä»¥é™ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ãŒå¯¾è±¡
        end_date: çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰ã€‚ã“ã®æ—¥ä»˜ä»¥å‰ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ãŒå¯¾è±¡
        source: è«–æ–‡ã‚½ãƒ¼ã‚¹ ("arxiv", "semantic_scholar", ã¾ãŸã¯ "both")
        limit: å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
    
    Returns:
        æ¤œç´¢çµæœã®è¦ç´„ã€‚è¦‹ã¤ã‹ã£ãŸè«–æ–‡æ•°ã¨å„è«–æ–‡ã®è©³ç´°ã‚’å«ã¿ã¾ã™
    
    ä¾‹:
        * search_papers_by_date_range("transformer", "2017-01-01", "2017-12-31") - 
          2017å¹´ï¼ˆTransformerãƒ¢ãƒ‡ãƒ«ãŒç™»å ´ã—ãŸå¹´ï¼‰ã«ç™ºè¡¨ã•ã‚ŒãŸTransformerã«é–¢ã™ã‚‹è«–æ–‡ã‚’æ¤œç´¢
        * search_papers_by_date_range("\"attention is all you need\"", "2017-06-01", "2017-07-30", source="arxiv") - 
          2017å¹´6-7æœˆã«ç™ºè¡¨ã•ã‚ŒãŸæœ‰åãªTransformerè«–æ–‡ã‚’æ¤œç´¢
        * search_papers_by_date_range("BERT language model", "2018-10-01", "2019-05-31") - 
          BERTãƒ¢ãƒ‡ãƒ«ãŒç™ºè¡¨ã•ã‚ŒãŸæ™‚æœŸã®BERTã«é–¢ã™ã‚‹è«–æ–‡ã‚’æ¤œç´¢
        * search_papers_by_date_range("diffusion model", "2023-01-01", "2023-12-31", limit=20) - 
          2023å¹´ã«ç™ºè¡¨ã•ã‚ŒãŸæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹è«–æ–‡ã‚’æœ€å¤§20ä»¶æ¤œç´¢
        * search_papers_by_date_range("quantum computing", "2020-01-01", "2022-12-31", source="arxiv", limit=10) - 
          2020å¹´ã‹ã‚‰2022å¹´ã¾ã§ã®é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«é–¢ã™ã‚‹è«–æ–‡ã‚’arXivã‹ã‚‰æœ€å¤§10ä»¶æ¤œç´¢
    """
    try:
        # å…¥åŠ›å€¤ã®æ¤œè¨¼
        try:
            start_datetime = datetime.strptime(start_date, "%Y-%m-%d")
            end_datetime = datetime.strptime(end_date, "%Y-%m-%d")
            
            if start_datetime > end_datetime:
                return "ã‚¨ãƒ©ãƒ¼: é–‹å§‹æ—¥ãŒçµ‚äº†æ—¥ã‚ˆã‚Šå¾Œã«ãªã£ã¦ã„ã¾ã™ã€‚"
            
            arxiv_start = start_datetime.strftime("%Y%m%d")
            arxiv_end = end_datetime.strftime("%Y%m%d")
        except ValueError:
            return "ã‚¨ãƒ©ãƒ¼: æ—¥ä»˜å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚YYYY-MM-DDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
        
        log_info(f"æ—¥ä»˜ç¯„å›²ã§è«–æ–‡æ¤œç´¢: '{query}', æœŸé–“ = {start_date}ï½{end_date}")
        papers = []
        
        # è«–æ–‡ã‚’æ¤œç´¢
        if source.lower() in ["arxiv", "both"]:
            arxiv_papers = await arxiv_client.search_by_date(query, arxiv_start, arxiv_end, limit)
            papers.extend(arxiv_papers)
            
        if source.lower() in ["semantic_scholar", "both"]:
            semantic_papers = await semantic_scholar_client.search_by_date(
                query, start_datetime.year, end_datetime.year, limit)
            semantic_papers = semantic_scholar_client.filter_papers_by_date(
                semantic_papers, start_datetime, end_datetime)
            papers.extend(semantic_papers)
        
        if not papers:
            return f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{query}ã€ã§æ—¥ä»˜ç¯„å›² {start_date} ã‹ã‚‰ {end_date} ã®é–“ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        saved_count = paper_db.save_papers(papers)
        
        summary = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{query}ã€ã§æ—¥ä»˜ç¯„å›² {start_date} ã‹ã‚‰ {end_date} ã®é–“ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ãŒ {len(papers)}ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n"
        summary += f"ãã®ã†ã¡{saved_count}ä»¶ãŒæ–°è¦ã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚\n\n"
        
        if papers:
            summary += "æ¤œç´¢çµæœ:\n"
            for i, paper in enumerate(papers, 1):
                published_date = paper.get("published_date", "ä¸æ˜")
                full_text = "ï¼ˆå…¨æ–‡ã‚ã‚Šï¼‰" if paper.get("full_text_available", 0) == 1 else ""
                summary += f"{i}. {paper['title']} ({paper['source']})\n"
                summary += f"   ç™ºè¡¨æ—¥: {published_date}\n"
                summary += f"   è‘—è€…: {paper['authors']}\n"
                summary += f"   URL: {paper['url']}\n"
                summary += f"   {full_text}\n\n"
        
        return summary
    except Exception as e:
        log_error(f"æ—¥ä»˜ç¯„å›²ã«ã‚ˆã‚‹è«–æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"æ—¥ä»˜ç¯„å›²ã«ã‚ˆã‚‹è«–æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def list_saved_papers_by_date(
    start_date: str,
    end_date: str,
    keyword: str = "",
    source: str = "",
    limit: int = 10
) -> str:
    """
    æŒ‡å®šã—ãŸæ—¥ä»˜ç¯„å›²å†…ã«ç™ºè¡¨ã•ã‚ŒãŸä¿å­˜æ¸ˆã¿ã®è«–æ–‡ã®ä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
    
    ã“ã®é–¢æ•°ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ—¢ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹è«–æ–‡ã®ä¸­ã‹ã‚‰ã€ç‰¹å®šã®æ—¥ä»˜ç¯„å›²å†…ã«
    ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ã‚’æ¤œç´¢ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚ç‰¹å®šã®æœŸé–“ã«ãŠã‘ã‚‹ç ”ç©¶å‹•å‘ã‚’æŠŠæ¡ã—ãŸã‚Šã€
    å¹´ä»£åˆ¥ã«è«–æ–‡ã‚’æ•´ç†ã™ã‚‹éš›ã«å½¹ç«‹ã¡ã¾ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ã‚½ãƒ¼ã‚¹ã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿ã‚‚å¯èƒ½ã§ã™ã€‚
    
    æ—¥ä»˜ç¯„å›²æ¤œç´¢ã®ç‰¹å¾´:
    - æ—¥ä»˜ã¯YYYY-MM-DDå½¢å¼ã§æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆä¾‹: 2023-01-01ï¼‰
    - é–‹å§‹æ—¥ã¨çµ‚äº†æ—¥ã‚’å«ã‚€ç¯„å›²å†…ã®è«–æ–‡ãŒæ¤œç´¢ã•ã‚Œã¾ã™
    - è«–æ–‡ã®ç™ºè¡¨æ—¥ï¼ˆpublished_dateï¼‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚Œã¾ã™
    - çµæœã¯ç™ºè¡¨æ—¥ã®æ–°ã—ã„é †ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã§ã‚½ãƒ¼ãƒˆã•ã‚Œã¾ã™
    - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ ã™ã‚‹ã¨ã€ã‚¿ã‚¤ãƒˆãƒ«ã‚„æ¦‚è¦ã€è‘—è€…åãªã©ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚‚åŒæ™‚ã«è¡Œãˆã¾ã™
    
    Args:
        start_date: é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰ã€‚ã“ã®æ—¥ä»˜ä»¥é™ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ãŒå¯¾è±¡
        end_date: çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰ã€‚ã“ã®æ—¥ä»˜ä»¥å‰ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ãŒå¯¾è±¡
        keyword: ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã®è¿½åŠ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆéƒ¨åˆ†ä¸€è‡´ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        source: ç‰¹å®šã®ã‚½ãƒ¼ã‚¹ï¼ˆ"arxiv"ã¾ãŸã¯"semantic_scholar"ï¼‰ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        limit: è¡¨ç¤ºã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
    
    Returns:
        æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ç¯„å›²å†…ã®ä¿å­˜æ¸ˆã¿è«–æ–‡ä¸€è¦§
    
    ä¾‹:
        * list_saved_papers_by_date("2023-01-01", "2023-12-31") - 2023å¹´ã®è«–æ–‡ã‚’ä¸€è¦§è¡¨ç¤º
        * list_saved_papers_by_date("2020-01-01", "2022-12-31", keyword="neural", source="arxiv") - 
          2020å¹´ã‹ã‚‰2022å¹´ã¾ã§ã®arXivã‹ã‚‰å–å¾—ã—ãŸã€Œneuralã€ã‚’å«ã‚€è«–æ–‡ã‚’è¡¨ç¤º
    """
    try:
        # å…¥åŠ›å€¤ã®æ¤œè¨¼
        try:
            start_datetime = datetime.strptime(start_date, "%Y-%m-%d")
            end_datetime = datetime.strptime(end_date, "%Y-%m-%d")
            
            if start_datetime > end_datetime:
                return "ã‚¨ãƒ©ãƒ¼: é–‹å§‹æ—¥ãŒçµ‚äº†æ—¥ã‚ˆã‚Šå¾Œã«ãªã£ã¦ã„ã¾ã™ã€‚"
        except ValueError:
            return "ã‚¨ãƒ©ãƒ¼: æ—¥ä»˜å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚YYYY-MM-DDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
        
        log_info(f"æ—¥ä»˜ç¯„å›²ã§ä¿å­˜æ¸ˆã¿è«–æ–‡ä¸€è¦§: æœŸé–“ = {start_date}ï½{end_date}, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ = '{keyword}'")
        # è«–æ–‡ã‚’æ¤œç´¢
        papers = paper_db.get_papers(
            keyword=keyword,
            source=source,
            limit=limit,
            date_from=start_date,
            date_to=end_date,
            sort_by="date"
        )
        
        if not papers:
            return f"æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ç¯„å›² {start_date} ã‹ã‚‰ {end_date} ã«åˆè‡´ã™ã‚‹è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        result = f"æ—¥ä»˜ç¯„å›² {start_date} ã‹ã‚‰ {end_date} ã®è«–æ–‡: åˆè¨ˆ {len(papers)} ä»¶\n\n"
        
        for paper in papers:
            full_text = "ï¼ˆå…¨æ–‡ã‚ã‚Šï¼‰" if paper["full_text_available"] else ""
            result += f"ã‚¿ã‚¤ãƒˆãƒ«: {paper['title']} {full_text}\n"
            result += f"è‘—è€…: {paper['authors']}\n"
            result += f"ç™ºè¡¨æ—¥: {paper['published_date']}\n"
            result += f"ã‚½ãƒ¼ã‚¹: {paper['source']}\n"
            result += f"URL: {paper['url']}\n"
            if paper["citation_count"] > 0:
                result += f"å¼•ç”¨æ•°: {paper['citation_count']}\n"
            if paper["venue"]:
                result += f"æ²è¼‰å…ˆ: {paper['venue']}\n"
            if "abstract" in paper:
                result += f"æ¦‚è¦: {paper['abstract'][:200]}...\n"
            result += f"åé›†æ—¥: {paper['collected_date']}\n"
            result += "-" * 50 + "\n"
        
        return result
    except Exception as e:
        log_error(f"æ—¥ä»˜ç¯„å›²ã«ã‚ˆã‚‹è«–æ–‡ä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"æ—¥ä»˜ç¯„å›²ã«ã‚ˆã‚‹è«–æ–‡ä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"