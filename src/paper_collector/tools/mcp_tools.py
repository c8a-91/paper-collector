"""
Model Context Protocol (MCP) ãƒ„ãƒ¼ãƒ«é–¢æ•°ã‚’å®šç¾©ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
è«–æ–‡ã®æ¤œç´¢ã€è¡¨ç¤ºã€ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãªã©ã®æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã€‚
"""
from typing import Any, List, Dict, Optional
import os
import json
import pandas as pd
import asyncio
from mcp.server.fastmcp import FastMCP
from datetime import datetime
from pathlib import Path

try:
    # ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã—ã¦å®Ÿè¡Œã™ã‚‹å ´åˆ
    from paper_collector.db.database import PaperDatabase
    from paper_collector.pdf.pdf_handler import extract_text_from_pdf
    from paper_collector.api.arxiv_client import ArxivClient
    from paper_collector.api.semantic_scholar_client import SemanticScholarClient
    from paper_collector.utils.file_utils import ensure_directory_exists
    from paper_collector.utils.config import config
except ImportError:
    # é–‹ç™ºç’°å¢ƒã§ç›´æ¥å®Ÿè¡Œã™ã‚‹å ´åˆï¼ˆç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
    from ..db.database import PaperDatabase
    from ..pdf.pdf_handler import extract_text_from_pdf
    from ..api.arxiv_client import ArxivClient
    from ..api.semantic_scholar_client import SemanticScholarClient
    from ..utils.file_utils import ensure_directory_exists
    from ..utils.config import config

# MCPã‚µãƒ¼ãƒãƒ¼ã®åˆæœŸåŒ–
mcp = FastMCP("paper-collector")

# è¨­å®šã‹ã‚‰ãƒ‘ã‚¹ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
DATA_DIR = config.get_data_dir()
PAPERS_DIR = config.get_papers_dir()
DB_PATH = config.get_db_path()
API_DELAY = config.get("api_delay", 1.0)

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ã‚’ç¢ºèª
ensure_directory_exists(DATA_DIR)
ensure_directory_exists(PAPERS_DIR)

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
paper_db = PaperDatabase(DB_PATH)
arxiv_client = ArxivClient(PAPERS_DIR, API_DELAY)
semantic_scholar_client = SemanticScholarClient(PAPERS_DIR, API_DELAY)

# ãƒ­ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
def log_info(message: str) -> None:
    """æƒ…å ±ãƒ­ã‚°ã‚’å‡ºåŠ›ã—ã¾ã™"""
    print(f"[INFO] {message}")

def log_error(message: str) -> None:
    """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‡ºåŠ›ã—ã¾ã™"""
    print(f"[ERROR] {message}")

@mcp.tool()
async def search_papers(query: str, source: str = "both", limit: int = 5) -> str:
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹è«–æ–‡æ¤œç´¢ã‚’è¡Œã„ã€çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã™ã€‚
    
    Args:
        query: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        source: è«–æ–‡ã‚½ãƒ¼ã‚¹ ("arxiv", "semantic_scholar", ã¾ãŸã¯ "both")
        limit: å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
    
    Returns:
        æ¤œç´¢çµæœã®è¦ç´„
    """
    try:
        papers = []
        
        if source.lower() in ["arxiv", "both"]:
            log_info(f"ArXivã§æ¤œç´¢: {query}")
            arxiv_papers = await arxiv_client.search(query, limit)
            papers.extend(arxiv_papers)
            
        if source.lower() in ["semantic_scholar", "both"]:
            log_info(f"Semantic Scholarã§æ¤œç´¢: {query}")
            semantic_papers = await semantic_scholar_client.search(query, limit)
            papers.extend(semantic_papers)
        
        saved_count = paper_db.save_papers(papers)
        
        summary = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{query}ã€ã§{len(papers)}ä»¶ã®è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n"
        summary += f"ãã®ã†ã¡{saved_count}ä»¶ãŒæ–°è¦ã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚\n\n"
        
        full_text_count = sum(1 for paper in papers if paper.get("full_text_available", 0) == 1)
        summary += f"å…¨æ–‡ãŒåˆ©ç”¨å¯èƒ½ãªè«–æ–‡: {full_text_count}ä»¶\n\n"
        
        if saved_count > 0:
            summary += "æ–°è¦è¿½åŠ ã•ã‚ŒãŸè«–æ–‡:\n"
            seen_ids = set()
            
            for paper in papers:
                paper_id = paper["paper_id"]
                if paper_id not in seen_ids:
                    seen_ids.add(paper_id)
                    
                    db_paper = paper_db.get_paper_by_id(paper_id)
                    if db_paper and db_paper["collected_date"] == datetime.now().strftime("%Y-%m-%d"):
                        full_text = "ï¼ˆå…¨æ–‡ã‚ã‚Šï¼‰" if paper.get("full_text_available", 0) == 1 else ""
                        summary += f"- {paper['title']} ({paper['source']}) {full_text}\n"
        
        return summary
    except Exception as e:
        log_error(f"è«–æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"è«–æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def search_papers_by_citations(
    query: str, 
    min_citations: int = 0, 
    source: str = "both", 
    limit: int = 5, 
    sort_by: str = "citations"
) -> str:
    """
    å¼•ç”¨æ•°ã‚’è€ƒæ…®ã—ãŸè«–æ–‡æ¤œç´¢ã‚’è¡Œã„ã€çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã™ã€‚
    
    Args:
        query: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        min_citations: æœ€å°å¼•ç”¨æ•°
        source: è«–æ–‡ã‚½ãƒ¼ã‚¹ ("arxiv", "semantic_scholar", ã¾ãŸã¯ "both")
        limit: å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
        sort_by: ã‚½ãƒ¼ãƒˆæ–¹æ³• ("relevance", "citations", "recency")
    
    Returns:
        æ¤œç´¢çµæœã®è¦ç´„
    """
    try:
        papers = []
        
        if source.lower() in ["arxiv", "both"]:
            log_info(f"ArXivã§å¼•ç”¨æ•°æ¤œç´¢: {query} (æœ€å°å¼•ç”¨æ•°: {min_citations})")
            arxiv_papers = await arxiv_client.search(query, limit, min_citations, sort_by)
            papers.extend(arxiv_papers)
            
        if source.lower() in ["semantic_scholar", "both"]:
            log_info(f"Semantic Scholarã§å¼•ç”¨æ•°æ¤œç´¢: {query} (æœ€å°å¼•ç”¨æ•°: {min_citations})")
            semantic_papers = await semantic_scholar_client.search(query, limit, min_citations, sort_by)
            papers.extend(semantic_papers)
        
        if sort_by == "citations":
            papers.sort(key=lambda x: x.get("citation_count", 0), reverse=True)
            papers = papers[:limit]
        
        saved_count = paper_db.save_papers(papers)
        
        summary = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{query}ã€ã§æœ€å°å¼•ç”¨æ•° {min_citations} ä»¥ä¸Šã®è«–æ–‡ãŒ {len(papers)}ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n"
        summary += f"ãã®ã†ã¡{saved_count}ä»¶ãŒæ–°è¦ã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚\n\n"
        
        if papers:
            summary += "æ¤œç´¢çµæœ:\n"
            for i, paper in enumerate(papers, 1):
                full_text = "ï¼ˆå…¨æ–‡ã‚ã‚Šï¼‰" if paper.get("full_text_available", 0) == 1 else ""
                summary += f"{i}. {paper['title']} ({paper['source']})\n"
                summary += f"   è‘—è€…: {paper['authors']}\n"
                summary += f"   å¼•ç”¨æ•°: {paper.get('citation_count', 0)}\n"
                if paper.get("venue"):
                    summary += f"   æ²è¼‰å…ˆ: {paper.get('venue')}\n"
                summary += f"   URL: {paper['url']}\n"
                summary += f"   {full_text}\n\n"
        else:
            summary += "æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ã«åˆè‡´ã™ã‚‹è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        return summary
    except Exception as e:
        log_error(f"å¼•ç”¨æ•°ã«ã‚ˆã‚‹è«–æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"å¼•ç”¨æ•°ã«ã‚ˆã‚‹è«–æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def list_saved_papers(
    keyword: str = "", 
    source: str = "", 
    limit: int = 10, 
    sort_by: str = "date", 
    sort_order: str = "desc",
    filter_has_fulltext: bool = False,
    min_citations: int = 0,
    format: str = "detailed",
    venue: str = "",
    date_from: str = "",
    date_to: str = ""
) -> str:
    """
    ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã®ä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
    
    Args:
        keyword: ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        source: ç‰¹å®šã®ã‚½ãƒ¼ã‚¹ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        limit: è¡¨ç¤ºã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
        sort_by: ã‚½ãƒ¼ãƒˆé †ï¼ˆ"date", "citations", "title"ï¼‰
        sort_order: ã‚½ãƒ¼ãƒˆæ–¹å‘ï¼ˆ"asc", "desc"ï¼‰
        filter_has_fulltext: å…¨æ–‡ãŒåˆ©ç”¨å¯èƒ½ãªè«–æ–‡ã®ã¿ã‚’è¡¨ç¤º
        min_citations: æœ€å°å¼•ç”¨æ•°
        format: è¡¨ç¤ºå½¢å¼ï¼ˆ"detailed", "compact", "csv"ï¼‰
        venue: ç‰¹å®šã®æ²è¼‰å…ˆã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        date_from: ã“ã®æ—¥ä»˜ä»¥é™ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
        date_to: ã“ã®æ—¥ä»˜ä»¥å‰ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
    
    Returns:
        ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã®ä¸€è¦§
    """
    try:
        log_info(f"ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã‚’ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: '{keyword}', ã‚½ãƒ¼ãƒˆ: {sort_by} {sort_order}")
        
        # Get papers based on search criteria
        papers = paper_db.get_papers(
            keyword, source, limit, sort_by, sort_order,
            filter_has_fulltext, min_citations, venue, date_from, date_to
        )
        
        if not papers:
            return "æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ã«åˆè‡´ã™ã‚‹è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # Format results based on requested format
        if format == "csv":
            return _format_papers_as_csv(papers)
        elif format == "compact":
            return _format_papers_as_compact(papers)
        else:
            return _format_papers_as_detailed(papers)
            
    except Exception as e:
        log_error(f"ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã®ä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã®ä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

def _format_papers_as_csv(papers: List[Dict[str, Any]]) -> str:
    """è«–æ–‡ãƒªã‚¹ãƒˆã‚’CSVå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™"""
    result = "paper_id,title,authors,source,url,citation_count,venue,published_date,collected_date,full_text_available\n"
    for paper in papers:
        result += f'"{paper["paper_id"]}","{paper["title"].replace("\"", "\"\"")}","{paper["authors"].replace("\"", "\"\"")}","{paper["source"]}","{paper["url"]}","{paper["citation_count"]}","{paper.get("venue", "").replace("\"", "\"\"")}","{paper["published_date"]}","{paper["collected_date"]}","{paper["full_text_available"]}"\n'
    return result

def _format_papers_as_compact(papers: List[Dict[str, Any]]) -> str:
    """è«–æ–‡ãƒªã‚¹ãƒˆã‚’ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™"""
    result = f"åˆè¨ˆ {len(papers)} ä»¶ã®è«–æ–‡:\n\n"
    for i, paper in enumerate(papers, 1):
        full_text = "ğŸ“„" if paper["full_text_available"] else ""
        citations = f"[å¼•ç”¨:{paper['citation_count']}]" if paper['citation_count'] > 0 else ""
        result += f"{i}. {paper['title']} {full_text} {citations}\n"
        result += f"   è‘—è€…: {paper['authors'][:50]}{'...' if len(paper['authors']) > 50 else ''}\n"
        result += f"   ç™ºè¡¨: {paper['published_date']} | åé›†: {paper['collected_date']}\n"
        result += f"   ã‚½ãƒ¼ã‚¹: {paper['source']}\n\n"
    return result

def _format_papers_as_detailed(papers: List[Dict[str, Any]]) -> str:
    """è«–æ–‡ãƒªã‚¹ãƒˆã‚’è©³ç´°å½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™"""
    result = f"åˆè¨ˆ {len(papers)} ä»¶ã®è«–æ–‡:\n\n"
    
    for paper in papers:
        full_text = "ï¼ˆå…¨æ–‡ã‚ã‚Šï¼‰" if paper["full_text_available"] else ""
        result += f"ã‚¿ã‚¤ãƒˆãƒ«: {paper['title']} {full_text}\n"
        result += f"è‘—è€…: {paper['authors']}\n"
        result += f"ã‚½ãƒ¼ã‚¹: {paper['source']}\n"
        result += f"URL: {paper['url']}\n"
        
        if paper["citation_count"] > 0:
            result += f"å¼•ç”¨æ•°: {paper['citation_count']}\n"
        
        if paper.get("venue"):
            result += f"æ²è¼‰å…ˆ: {paper['venue']}\n"
        
        # ç™ºè¡¨æ—¥ã‚’è¿½åŠ 
        if paper.get("published_date"):
            result += f"ç™ºè¡¨æ—¥: {paper['published_date']}\n"
        
        # æ¦‚è¦ã¯é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if paper.get("abstract"):
            abstract_preview = paper["abstract"][:200].replace("\n", " ")
            result += f"æ¦‚è¦: {abstract_preview}{'...' if len(paper['abstract']) > 200 else ''}\n"
        
        result += f"åé›†æ—¥: {paper['collected_date']}\n"
        result += f"ID: {paper['paper_id']}\n"
        result += "-" * 50 + "\n"
        
    return result

@mcp.tool()
async def rank_papers_by_citations(keyword: str = "", limit: int = 10) -> str:
    """
    ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã‚’å¼•ç”¨æ•°ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚
    
    Args:
        keyword: ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        limit: è¡¨ç¤ºã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
    
    Returns:
        å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    """
    try:
        log_info(f"è«–æ–‡ã®å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: '{keyword}'")
        papers = paper_db.get_papers(
            keyword=keyword,
            limit=limit,
            sort_by="citations",
            sort_order="desc"
        )
        
        if not papers:
            return "æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ã«åˆè‡´ã™ã‚‹è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        result = f"å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆ{keyword if keyword else 'å…¨ã¦'}ï¼‰:\n\n"
        
        for i, paper in enumerate(papers, 1):
            result += f"{i}. {paper['title']}\n"
            result += f"   è‘—è€…: {paper['authors']}\n"
            result += f"   å¼•ç”¨æ•°: {paper['citation_count']}\n"
            if paper['venue']:
                result += f"   æ²è¼‰å…ˆ: {paper['venue']}\n"
            result += f"   ã‚½ãƒ¼ã‚¹: {paper['source']}\n"
            result += f"   URL: {paper['url']}\n"
            result += f"   åé›†æ—¥: {paper['collected_date']}\n"
            result += "-" * 50 + "\n"
        
        return result
    except Exception as e:
        log_error(f"å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def list_papers_by_venue(venue: str = "", limit: int = 10) -> str:
    """
    ç‰¹å®šã®æ²è¼‰å…ˆï¼ˆã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ã‚„ä¼šè­°ï¼‰ã®è«–æ–‡ä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
    
    Args:
        venue: æ²è¼‰å…ˆã®åå‰ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
        limit: è¡¨ç¤ºã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
    
    Returns:
        è«–æ–‡ä¸€è¦§
    """
    try:
        log_info(f"æ²è¼‰å…ˆã§ã®è«–æ–‡æ¤œç´¢: '{venue}'")
        papers = paper_db.get_papers_by_venue(venue, limit)
        
        if not papers:
            return "æŒ‡å®šã•ã‚ŒãŸæ²è¼‰å…ˆã®è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        venues = {}
        for paper in papers:
            v = paper['venue']
            if v not in venues:
                venues[v] = []
            venues[v].append(paper)
        
        result = f"æ²è¼‰å…ˆåˆ¥è«–æ–‡ä¸€è¦§ï¼ˆ{venue if venue else 'å…¨ã¦'}ï¼‰:\n\n"
        
        for v, v_papers in venues.items():
            result += f"ã€{v}ã€‘- {len(v_papers)}ä»¶\n"
            for paper in v_papers:
                result += f"- {paper['title']}\n"
                result += f"  è‘—è€…: {paper['authors']}\n"
                result += f"  å¼•ç”¨æ•°: {paper['citation_count']}\n"
                result += f"  URL: {paper['url']}\n"
            result += "-" * 50 + "\n"
        
        return result
    except Exception as e:
        log_error(f"æ²è¼‰å…ˆåˆ¥è«–æ–‡ä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"æ²è¼‰å…ˆåˆ¥è«–æ–‡ä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

# æ®‹ã‚Šã®é–¢æ•°ã¯åŒæ§˜ã«ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™
@mcp.tool()
async def list_top_venues(limit: int = 10) -> str:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹è«–æ–‡ã®ãƒˆãƒƒãƒ—æ²è¼‰å…ˆä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
    
    Args:
        limit: è¡¨ç¤ºã™ã‚‹æ²è¼‰å…ˆã®æœ€å¤§æ•°
    
    Returns:
        æ²è¼‰å…ˆä¸€è¦§
    """
    try:
        log_info(f"ãƒˆãƒƒãƒ—æ²è¼‰å…ˆä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ä¸Šä½ {limit} ä»¶")
        venues = paper_db.get_top_venues(limit)
        
        if not venues:
            return "æ²è¼‰å…ˆæƒ…å ±ãŒã‚ã‚‹è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        result = "ãƒˆãƒƒãƒ—æ²è¼‰å…ˆä¸€è¦§ï¼ˆå¹³å‡å¼•ç”¨æ•°é †ï¼‰:\n\n"
        
        for i, (venue, paper_count, avg_citations, max_citations) in enumerate(venues, 1):
            result += f"{i}. {venue}\n"
            result += f"   è«–æ–‡æ•°: {paper_count}ä»¶\n"
            result += f"   å¹³å‡å¼•ç”¨æ•°: {avg_citations:.1f}\n"
            result += f"   æœ€å¤§å¼•ç”¨æ•°: {max_citations}\n"
            result += "-" * 50 + "\n"
        
        return result
    except Exception as e:
        log_error(f"ãƒˆãƒƒãƒ—æ²è¼‰å…ˆä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"ãƒˆãƒƒãƒ—æ²è¼‰å…ˆä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def get_paper_details(paper_id: str) -> str:
    """
    ç‰¹å®šã®è«–æ–‡ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚paper_idã¾ãŸã¯è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢ã§ãã¾ã™ã€‚
    
    Args:
        paper_id: è«–æ–‡ã®IDã¾ãŸã¯ã‚¿ã‚¤ãƒˆãƒ«
    
    Returns:
        è«–æ–‡ã®è©³ç´°æƒ…å ±
    """
    try:
        log_info(f"è«–æ–‡è©³ç´°ã‚’æ¤œç´¢: '{paper_id}'")
        paper = paper_db.get_paper_by_id(paper_id)
        
        if not paper:
            return f"ID ã¾ãŸã¯ ã‚¿ã‚¤ãƒˆãƒ« '{paper_id}' ã®è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        full_text = "ï¼ˆå…¨æ–‡ã‚ã‚Šï¼‰" if paper["full_text_available"] else "ï¼ˆå…¨æ–‡ãªã—ï¼‰"
        result = f"ã‚¿ã‚¤ãƒˆãƒ«: {paper['title']} {full_text}\n"
        result += f"è‘—è€…: {paper['authors']}\n"
        result += f"å‡ºç‰ˆå¹´: {paper['published_date']}\n"
        result += f"ã‚½ãƒ¼ã‚¹: {paper['source']}\n"
        result += f"URL: {paper['url']}\n"
        
        if paper["citation_count"] > 0:
            result += f"å¼•ç”¨æ•°: {paper['citation_count']}\n"
        
        if paper["venue"]:
            result += f"æ²è¼‰å…ˆ: {paper['venue']}\n"
        
        if paper["full_text_available"] and paper["pdf_path"]:
            result += f"PDF: {paper['pdf_path']}\n"
        
        result += f"åé›†æ—¥: {paper['collected_date']}\n"
        result += f"\næ¦‚è¦:\n{paper['abstract']}\n"
        
        if paper['keywords']:
            result += f"\nã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {paper['keywords']}\n"
        
        return result
    except Exception as e:
        log_error(f"è«–æ–‡è©³ç´°è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"è«–æ–‡è©³ç´°è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def get_paper_full_text(paper_id: str, max_length: int = 1000000) -> str:
    """
    ä¿å­˜ã•ã‚ŒãŸPDFã‹ã‚‰è«–æ–‡ã®å…¨æ–‡ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Args:
        paper_id: è«–æ–‡ã®IDã¾ãŸã¯ã‚¿ã‚¤ãƒˆãƒ«
        max_length: è¿”ã™ãƒ†ã‚­ã‚¹ãƒˆã®æœ€å¤§é•·ã•ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000000æ–‡å­—ï¼‰
    
    Returns:
        è«–æ–‡ã®å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆ
    """
    try:
        log_info(f"è«–æ–‡å…¨æ–‡ã‚’å–å¾—: '{paper_id}'")
        paper = paper_db.get_paper_by_id(paper_id)
        
        if not paper:
            return f"ID ã¾ãŸã¯ ã‚¿ã‚¤ãƒˆãƒ« '{paper_id}' ã®è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        if not paper["full_text_available"] or not paper["pdf_path"]:
            return f"è«–æ–‡ '{paper['title']}' ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        full_text = paper["full_text"] if paper["full_text"] is not None else None
        
        if not full_text:
            log_info(f"PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º: {paper['pdf_path']}")
            full_text = extract_text_from_pdf(paper["pdf_path"])
            
            if full_text:
                paper_db.save_full_text(paper["paper_id"], full_text)
            else:
                return f"è«–æ–‡ '{paper['title']}' ã®PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    
        if len(full_text) > max_length:
            full_text = full_text[:max_length] + f"\n\n... (ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã„ãŸã‚åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸã€‚å…¨æ–‡ã¯ {len(full_text)} æ–‡å­—ã‚ã‚Šã¾ã™)"
    
        return f"ã‚¿ã‚¤ãƒˆãƒ«: {paper['title']}\nè‘—è€…: {paper['authors']}\n\nå…¨æ–‡:\n{full_text}"
    except Exception as e:
        log_error(f"è«–æ–‡å…¨æ–‡å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"è«–æ–‡å…¨æ–‡å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def search_full_text(query: str, limit: int = 5) -> str:
    """
    è«–æ–‡ã®å…¨æ–‡ã‹ã‚‰ç‰¹å®šã®æ–‡å­—åˆ—ã‚’æ¤œç´¢ã—ã¾ã™ã€‚
    
    Args:
        query: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        limit: è¡¨ç¤ºã™ã‚‹æœ€å¤§çµæœæ•°
    
    Returns:
        æ¤œç´¢çµæœ
    """
    try:
        log_info(f"è«–æ–‡å…¨æ–‡ã‚’æ¤œç´¢: '{query}'")
        papers = paper_db.get_papers(filter_has_fulltext=True, limit=100)  # ã‚ˆã‚Šå¤šãã®è«–æ–‡ã‚’å–å¾—ã—ã¦æ¤œç´¢
        
        if not papers:
            return "å…¨æ–‡ãŒåˆ©ç”¨å¯èƒ½ãªè«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        results = []
        
        for paper in papers:
            full_text = paper["full_text"] if paper["full_text"] is not None else None
            if not full_text and paper["pdf_path"]:
                full_text = extract_text_from_pdf(paper["pdf_path"])
                if full_text:
                    paper_db.save_full_text(paper["paper_id"], full_text)
            
            if full_text and query.lower() in full_text.lower():
                index = full_text.lower().find(query.lower())
                start = max(0, index - 100)
                end = min(len(full_text), index + len(query) + 100)
                
                context = full_text[start:end].replace("\n", " ")
                if start > 0:
                    context = "..." + context
                if end < len(full_text):
                    context = context + "..."
                
                results.append({
                    "paper_id": paper["paper_id"],
                    "title": paper["title"],
                    "authors": paper["authors"],
                    "context": context
                })
                
                if len(results) >= limit:
                    break
        
        if not results:
            return f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{query}' ã‚’å«ã‚€è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        result_text = f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{query}' ã‚’å«ã‚€è«–æ–‡: {len(results)}ä»¶\n\n"
        
        for i, res in enumerate(results, 1):
            result_text += f"{i}. ã‚¿ã‚¤ãƒˆãƒ«: {res['title']}\n"
            result_text += f"   è‘—è€…: {res['authors']}\n"
            result_text += f"   ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {res['context']}\n"
            result_text += f"   (Paper ID: {res['paper_id']})\n\n"
        
        return result_text
    except Exception as e:
        log_error(f"å…¨æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"å…¨æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def export_summaries(format: str = "json") -> str:
    """
    ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã®è¦ç´„ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚
    
    Args:
        format: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼ ("json" ã¾ãŸã¯ "csv")
    
    Returns:
        ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®çµæœ
    """
    try:
        log_info(f"è«–æ–‡è¦ç´„ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: å½¢å¼ = {format}")
        papers = paper_db.get_papers(limit=1000)  # æœ€å¤§1000ä»¶ã®è«–æ–‡ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        
        if not papers:
            return "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if format.lower() == "json":
            export_path = os.path.join(DATA_DIR, f"paper_summaries_{timestamp}.json")
            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(papers, f, ensure_ascii=False, indent=2)
        else:
            export_path = os.path.join(DATA_DIR, f"paper_summaries_{timestamp}.csv")
            df = pd.DataFrame(papers)
            df.to_csv(export_path, index=False)
        
        return f"è«–æ–‡è¦ç´„ã‚’ {export_path} ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚"
    except Exception as e:
        log_error(f"è¦ç´„ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"è¦ç´„ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def search_papers_by_date_range(
    query: str,
    start_date: str,
    end_date: str,
    source: str = "both",
    limit: int = 5
) -> str:
    """
    æŒ‡å®šã—ãŸæ—¥ä»˜ç¯„å›²å†…ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ã‚’æ¤œç´¢ã—ã¾ã™ã€‚
    
    Args:
        query: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        start_date: é–‹å§‹æ—¥ (YYYY-MM-DDå½¢å¼)
        end_date: çµ‚äº†æ—¥ (YYYY-MM-DDå½¢å¼)
        source: è«–æ–‡ã‚½ãƒ¼ã‚¹ ("arxiv", "semantic_scholar", ã¾ãŸã¯ "both")
        limit: å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
    
    Returns:
        æ¤œç´¢çµæœã®è¦ç´„
    """
    try:
        # å…¥åŠ›å€¤ã®æ¤œè¨¼
        try:
            start_datetime = datetime.strptime(start_date, "%Y-%m-%d")
            end_datetime = datetime.strptime(end_date, "%Y-%m-%d")
            
            if start_datetime > end_datetime:
                return "ã‚¨ãƒ©ãƒ¼: é–‹å§‹æ—¥ãŒçµ‚äº†æ—¥ã‚ˆã‚Šå¾Œã«ãªã£ã¦ã„ã¾ã™ã€‚"
            
            arxiv_start = start_datetime.strftime("%Y%m%d")
            arxiv_end = end_datetime.strftime("%Y%m%d")
        except ValueError:
            return "ã‚¨ãƒ©ãƒ¼: æ—¥ä»˜å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚YYYY-MM-DDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
        
        log_info(f"æ—¥ä»˜ç¯„å›²ã§è«–æ–‡æ¤œç´¢: '{query}', æœŸé–“ = {start_date}ï½{end_date}")
        papers = []
        
        # è«–æ–‡ã‚’æ¤œç´¢
        if source.lower() in ["arxiv", "both"]:
            arxiv_papers = await arxiv_client.search_by_date(query, arxiv_start, arxiv_end, limit)
            papers.extend(arxiv_papers)
            
        if source.lower() in ["semantic_scholar", "both"]:
            semantic_papers = await semantic_scholar_client.search_by_date(
                query, start_datetime.year, end_datetime.year, limit)
            semantic_papers = semantic_scholar_client.filter_papers_by_date(
                semantic_papers, start_datetime, end_datetime)
            papers.extend(semantic_papers)
        
        if not papers:
            return f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{query}ã€ã§æ—¥ä»˜ç¯„å›² {start_date} ã‹ã‚‰ {end_date} ã®é–“ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        saved_count = paper_db.save_papers(papers)
        
        summary = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{query}ã€ã§æ—¥ä»˜ç¯„å›² {start_date} ã‹ã‚‰ {end_date} ã®é–“ã«ç™ºè¡¨ã•ã‚ŒãŸè«–æ–‡ãŒ {len(papers)}ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚\n"
        summary += f"ãã®ã†ã¡{saved_count}ä»¶ãŒæ–°è¦ã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚\n\n"
        
        if papers:
            summary += "æ¤œç´¢çµæœ:\n"
            for i, paper in enumerate(papers, 1):
                published_date = paper.get("published_date", "ä¸æ˜")
                full_text = "ï¼ˆå…¨æ–‡ã‚ã‚Šï¼‰" if paper.get("full_text_available", 0) == 1 else ""
                summary += f"{i}. {paper['title']} ({paper['source']})\n"
                summary += f"   ç™ºè¡¨æ—¥: {published_date}\n"
                summary += f"   è‘—è€…: {paper['authors']}\n"
                summary += f"   URL: {paper['url']}\n"
                summary += f"   {full_text}\n\n"
        
        return summary
    except Exception as e:
        log_error(f"æ—¥ä»˜ç¯„å›²ã«ã‚ˆã‚‹è«–æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"æ—¥ä»˜ç¯„å›²ã«ã‚ˆã‚‹è«–æ–‡æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

@mcp.tool()
async def list_saved_papers_by_date(
    start_date: str,
    end_date: str,
    keyword: str = "",
    source: str = "",
    limit: int = 10
) -> str:
    """
    æŒ‡å®šã—ãŸæ—¥ä»˜ç¯„å›²å†…ã«ç™ºè¡¨ã•ã‚ŒãŸä¿å­˜æ¸ˆã¿ã®è«–æ–‡ã®ä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
    
    Args:
        start_date: é–‹å§‹æ—¥ (YYYY-MM-DDå½¢å¼)
        end_date: çµ‚äº†æ—¥ (YYYY-MM-DDå½¢å¼)
        keyword: ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        source: ç‰¹å®šã®ã‚½ãƒ¼ã‚¹ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        limit: è¡¨ç¤ºã™ã‚‹è«–æ–‡ã®æœ€å¤§æ•°
    
    Returns:
        ä¿å­˜ã•ã‚ŒãŸè«–æ–‡ã®ä¸€è¦§
    """
    try:
        # å…¥åŠ›å€¤ã®æ¤œè¨¼
        try:
            start_datetime = datetime.strptime(start_date, "%Y-%m-%d")
            end_datetime = datetime.strptime(end_date, "%Y-%m-%d")
            
            if start_datetime > end_datetime:
                return "ã‚¨ãƒ©ãƒ¼: é–‹å§‹æ—¥ãŒçµ‚äº†æ—¥ã‚ˆã‚Šå¾Œã«ãªã£ã¦ã„ã¾ã™ã€‚"
        except ValueError:
            return "ã‚¨ãƒ©ãƒ¼: æ—¥ä»˜å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚YYYY-MM-DDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
        
        log_info(f"æ—¥ä»˜ç¯„å›²ã§ä¿å­˜æ¸ˆã¿è«–æ–‡ä¸€è¦§: æœŸé–“ = {start_date}ï½{end_date}, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ = '{keyword}'")
        # è«–æ–‡ã‚’æ¤œç´¢
        papers = paper_db.get_papers(
            keyword=keyword,
            source=source,
            limit=limit,
            date_from=start_date,
            date_to=end_date,
            sort_by="date"
        )
        
        if not papers:
            return f"æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ç¯„å›² {start_date} ã‹ã‚‰ {end_date} ã«åˆè‡´ã™ã‚‹è«–æ–‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        result = f"æ—¥ä»˜ç¯„å›² {start_date} ã‹ã‚‰ {end_date} ã®è«–æ–‡: åˆè¨ˆ {len(papers)} ä»¶\n\n"
        
        for paper in papers:
            full_text = "ï¼ˆå…¨æ–‡ã‚ã‚Šï¼‰" if paper["full_text_available"] else ""
            result += f"ã‚¿ã‚¤ãƒˆãƒ«: {paper['title']} {full_text}\n"
            result += f"è‘—è€…: {paper['authors']}\n"
            result += f"ç™ºè¡¨æ—¥: {paper['published_date']}\n"
            result += f"ã‚½ãƒ¼ã‚¹: {paper['source']}\n"
            result += f"URL: {paper['url']}\n"
            if paper["citation_count"] > 0:
                result += f"å¼•ç”¨æ•°: {paper['citation_count']}\n"
            if paper["venue"]:
                result += f"æ²è¼‰å…ˆ: {paper['venue']}\n"
            if "abstract" in paper:
                result += f"æ¦‚è¦: {paper['abstract'][:200]}...\n"
            result += f"åé›†æ—¥: {paper['collected_date']}\n"
            result += "-" * 50 + "\n"
        
        return result
    except Exception as e:
        log_error(f"æ—¥ä»˜ç¯„å›²ã«ã‚ˆã‚‹è«–æ–‡ä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"æ—¥ä»˜ç¯„å›²ã«ã‚ˆã‚‹è«–æ–‡ä¸€è¦§è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"